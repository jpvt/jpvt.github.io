<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Autoenconder | Jo√£o Pedro Vasconcelos</title>
    <link>https://jpvt.github.io/tag/autoenconder/</link>
      <atom:link href="https://jpvt.github.io/tag/autoenconder/index.xml" rel="self" type="application/rss+xml" />
    <description>Autoenconder</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jpvt.github.io/media/icon_hu6e0ba196f4e113be19f679f5e0d6caf1_39320_512x512_fill_lanczos_center_2.png</url>
      <title>Autoenconder</title>
      <link>https://jpvt.github.io/tag/autoenconder/</link>
    </image>
    
    <item>
      <title>Denoising Documents with Computer Vision and Digital Image Processing</title>
      <link>https://jpvt.github.io/post/documentcleanup/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/post/documentcleanup/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Requirements&lt;/li&gt;
&lt;li&gt;Median Filtering&lt;/li&gt;
&lt;li&gt;AutoEncoder&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-description&#34;&gt;1. Description&lt;/h2&gt;
&lt;p&gt;Many image processing applications make use of digitalized textual data. However, the presence of any type of noise can create difficulties in post-processing information, such as on OCR detection. To improve the information manipulation on such data, a previous image processing step is required.&lt;/p&gt;
&lt;p&gt;In light of this idea, a set of text paragraphs containing plain English language was collected. Different font styles, size, and background noise level were arranged to simulate the a variety of scenarios.&lt;/p&gt;
&lt;p&gt;The objective of this article is to evaluate the possible image processing methods that could fix the text samples. Note that the samples have a different type of background noise and present a set of text fonts. Therefore, the candidate should provide a flexible algorithm that can correctly detect what is text characters and background noise, offering a clean version of each text paragraph as result.&lt;/p&gt;
&lt;h2 id=&#34;2-requirements&#34;&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;!pip install numpy
!pip install scipy
!pip install pillow
!pip install opencv-python
!pip install matplotlib
!pip install tensorflow
!pip install scikit-learn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;########################################################
from os import listdir
from os.path import isfile, join
########################################################
import numpy as np
from scipy import signal
from PIL import Image
import cv2
import matplotlib.pyplot as plt
########################################################
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model, layers
from sklearn.model_selection import train_test_split
########################################################
# Comment the following code if you don&#39;t have tensorflow-gpu installed in your enviroment
gpu = len(tf.config.list_physical_devices(&#39;GPU&#39;))&amp;gt;0
print(&amp;quot;GPU is&amp;quot;, &amp;quot;available&amp;quot; if gpu else &amp;quot;NOT AVAILABLE&amp;quot;)
physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;)
tf.config.experimental.set_memory_growth(physical_devices[0], True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GPU is available
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-median-filtering&#34;&gt;3. Median Filtering&lt;/h2&gt;
&lt;p&gt;A simple way to solve this problem is using classic digital image processing techniques since it will not be necessary to train any machine learning algorithms, which usually require more data, time, and better hardware. So, as a first answer, I will apply the median filter to get the background of the image, then I will subtract it from the original image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Denoiser:
    &amp;quot;&amp;quot;&amp;quot;
    Class that creates and object capable of colect dirty images and partially clean some of its noise
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self):
        self.input_image = None
        self.output_image = None
    
    def load_image(self, path):
        # Load an image from the path and return it as a numpy array
        return np.asarray(Image.open(path))/255.0
    
    def save_image(self, dest_path, image):
        #  Take some image array(image) and save it in the destination(dest_path)
        img_arr = np.asarray(image*255.0, dtype=np.uint8)
        Image.fromarray(img_arr).save(dest_path)
        
    def denoiser_pipeline(self, image):
        # Collect background
        background = signal.medfilt2d(image, 11)
        # Select anything that is darker than the background
        foreground_mask = image &amp;lt; background - 0.1
        # Return black for anything that is darker than the background or white otherwise
        output_image = np.where(foreground_mask, 0.0, 1.0)
        
        return output_image
        
    def clean(self, image_path, dest_path):
        # Load the input image
        self.input_image = self.load_image(image_path)
        # Process the image and load it in the output
        self.output_image = self.denoiser_pipeline(self.input_image)
        # Saves the output
        self.save_image(dest_path, self.output_image)
        
    def show(self):
        # Show the last image filtered in a kernel
        in_img = np.asarray(self.input_image*255.0, dtype = np.uint8)
        out_img = np.asarray(self.output_image*255.0, dtype = np.uint8)
        
        fig , axs = plt.subplots(1,2, figsize = (16,16))
        axs[0].imshow(Image.fromarray(in_img), cmap = &#39;gray&#39;)
        axs[1].imshow(Image.fromarray(out_img), cmap = &#39;gray&#39;)
        axs[0].axis(&#39;off&#39;)
        axs[1].axis(&#39;off&#39;)
        plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_datapath = &#39;noisy_data/&#39;
output_datapath = &#39;output_median/&#39;

files = [f for f in listdir(input_datapath) if isfile(join(input_datapath, f))]

denoiser = Denoiser()

for f in files:
    
    denoiser.clean(input_datapath + f, output_datapath + f)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;denoiser.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Analyzing the processed images is possible to see that the algorithm works relatively well, considering its simplicity. It is possible to use it in scenarios without many resources to use more sophisticated and modern solutions.&lt;/p&gt;
&lt;p&gt;The results are available in the output_median directory.&lt;/p&gt;
&lt;h2 id=&#34;4-autoencoder&#34;&gt;4. AutoEncoder&lt;/h2&gt;
&lt;p&gt;A more sophisticated answer to the problem is using the &amp;ldquo;magic&amp;rdquo; of Deep Learning. For this, I will use an Autoencoder network, which is composed of an encoder and a decoder. The encoder compresses the data to a smaller representation. The decoder reconstructs this representation to obtain an output close to the input. During the process, the autoencoder learns the most important features that make up the data.&lt;/p&gt;
&lt;p&gt;Autoencoders can reduce image noise by providing noisy data as an input and outputting the same data without noise. Then, the autoencoder can learn how to filter similar data efficiently.&lt;/p&gt;
&lt;p&gt;But how is it possible to apply this to the proposed problem?&lt;/p&gt;
&lt;p&gt;As stated in the test description, cleaning document images is a well-documented problem. Then it is possible to find databases that provide images with background noise and their respective clean version with ease. After looking for a database that fits the task, I trained an autoencoder using its dirty data as input and its clean images as output. So, I used the network to filter the dirty inputs presented by the challenge.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simple image process to standardize our data
def process_image(path):
    img = cv2.imread(path)
    img = np.asarray(img, dtype=&amp;quot;float32&amp;quot;)
    img = cv2.resize(img, (540, 420))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = img/255.0
    img = np.reshape(img, (420, 540, 1))
    
    return img
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading the training images
noisy_path = &#39;train/&#39;
clean_path = &#39;train_cleaned/&#39;
test_path = &#39;test/&#39;

noisy_dir = listdir(noisy_path)
x_paths = [noisy_path+x for x in noisy_dir]

clean_dir = listdir(clean_path)
y_paths = [clean_path+y for y in clean_dir]

size = (420,540)


X = []
y = []

for image in x_paths:
    
    img = process_image(image)
    X.append(img)

for label in y_paths:
    
    img = process_image(label)
    y.append(img)
    


X = np.array(X)
y = np.array(y)


fig, axs = plt.subplots(1,2, figsize = (16,16))

axs[0].set_title(&#39;Dirty Image&#39;)
axs[0].imshow(X[0][:,:,0], cmap = &#39;gray&#39;)
axs[0].axis(&#39;off&#39;)

axs[1].set_title(&#39;Clean Image&#39;)
axs[1].imshow(y[0][:,:,0], cmap = &#39;gray&#39;)
axs[1].axis(&#39;off&#39;)

plt.show()


print(&amp;quot;Size of X : &amp;quot;, X.shape)
print(&amp;quot;Size of Y : &amp;quot;, y.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Size of X :  (144, 420, 540, 1)
Size of Y :  (144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the construction of the autoencoder, I will use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional layers to extract the significant features of the images;&lt;/li&gt;
&lt;li&gt;Max-pooling for data compression;&lt;/li&gt;
&lt;li&gt;Up-sampling for restoring the data dimension;&lt;/li&gt;
&lt;li&gt;Batch normalization to reduce the difference in the distribution of activations between the layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autoencoder():
    
    
    input_layer = layers.Input(shape=(420,540,1), name= &#39;Image_Input&#39;)
    
    # Encoder
    
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_1&#39;)(input_layer)
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_2&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.MaxPooling2D((2,2), padding = &#39;same&#39;, name=&#39;Pooling_Layer_3&#39;)(x)
    
    
    # Decoder
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_5&#39;)(x)
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_6&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.UpSampling2D((2,2), name = &#39;UpSampling_Layer_3&#39;)(x)
    
    output_layer = layers.Conv2D(1, (3,3), activation = &#39;sigmoid&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_7&#39;)(x)
    
    # Model
    
    autoencoder = Model(inputs = [input_layer], outputs = [output_layer])
    autoencoder.compile(optimizer = &#39;adam&#39;, loss=&#39;mean_squared_error&#39;, metrics=[&#39;mae&#39;])
    
    return autoencoder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ae = autoencoder()
ae.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Image_Input (InputLayer)     [(None, 420, 540, 1)]     0         
_________________________________________________________________
Convolutional_Layer_1 (Conv2 (None, 420, 540, 32)      320       
_________________________________________________________________
Convolutional_Layer_2 (Conv2 (None, 420, 540, 64)      18496     
_________________________________________________________________
batch_normalization (BatchNo (None, 420, 540, 64)      256       
_________________________________________________________________
Pooling_Layer_3 (MaxPooling2 (None, 210, 270, 64)      0         
_________________________________________________________________
Convolutional_Layer_5 (Conv2 (None, 210, 270, 64)      36928     
_________________________________________________________________
Convolutional_Layer_6 (Conv2 (None, 210, 270, 32)      18464     
_________________________________________________________________
batch_normalization_1 (Batch (None, 210, 270, 32)      128       
_________________________________________________________________
UpSampling_Layer_3 (UpSampli (None, 420, 540, 32)      0         
_________________________________________________________________
Convolutional_Layer_7 (Conv2 (None, 420, 540, 1)       289       
=================================================================
Total params: 74,881
Trainable params: 74,689
Non-trainable params: 192
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;keras.utils.plot_model(ae, &amp;quot;assets/autoencoder.png&amp;quot;, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)
print(&amp;quot;Total number of training samples: &amp;quot;, X_train.shape)
print(&amp;quot;Total number of validation samples: &amp;quot;, X_valid.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of training samples:  (129, 420, 540, 1)
Total number of validation samples:  (15, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, patience=20)

hist = ae.fit(X_train, y_train, epochs=50, batch_size=2, validation_data=(X_valid, y_valid), callbacks = [callback])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_loss = hist.history[&#39;loss&#39;]
epoch_val_loss = hist.history[&#39;val_loss&#39;]
epoch_mae = hist.history[&#39;mae&#39;]
epoch_val_mae = hist.history[&#39;val_mae&#39;]

plt.figure(figsize=(20,6))
plt.subplot(1,2,1)
plt.plot(range(0,len(epoch_loss)), epoch_loss, &#39;b-&#39;, linewidth=2, label=&#39;Train Loss&#39;)
plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, &#39;r-&#39;, linewidth=2, label=&#39;Val Loss&#39;)
plt.title(&#39;Evolution of loss on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.subplot(1,2,2)
plt.plot(range(0,len(epoch_mae)), epoch_mae, &#39;b-&#39;, linewidth=2, label=&#39;Train MAE&#39;)
plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, &#39;r-&#39;, linewidth=2,label=&#39;Val MAE&#39;)
plt.title(&#39;Evolution of MAE on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The neural network seems to have learned to solve the problem well for the data sample provided so far, as you can see in the graphs.&lt;/p&gt;
&lt;p&gt;However, this does not mean that the network has learned well for images it has never seen, different noises, rotated texts.&lt;/p&gt;
&lt;p&gt;Then we will test the images on the noisy data provided by the test to check if the algorithm used was efficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_path = &#39;noisy_data/&#39;
test_dir = listdir(test_path)
test_paths = [test_path+x for x in test_dir]

X_test = []
for image in test_paths:
    
    img = process_image(image)
    X_test.append(img)
    
X_test = np.array(X_test)
print(X_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_test = ae.predict(X_test, batch_size=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,25))
for i in range(0,8,2):
    plt.subplot(4,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(X_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Noisy image: {}&#39;.format(test_paths[i]))
    
    plt.subplot(4,2,i+2)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(Y_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Denoised by autoencoder: {}&#39;.format(test_paths[i]))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the images above, the approached strategy was very efficient in removing the noise from the data provided. It seems that the model has learned to distinguish the background from the images well, even with different types of noise and rotated texts.&lt;/p&gt;
&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/denoising-dirty-documents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising Dirty Documents Kaggle Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_14_02_auto_encode.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising with AutoEncoders - T81-558: Applications of Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/michalbrezk/denoise-images-using-autoencoders-tf-keras&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoise images using Autoencoder[TF, Keras] &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
