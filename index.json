[{"authors":null,"categories":null,"content":"Hey, I\u0026rsquo;m João Pedro.\nI\u0026rsquo;m from São Paulo, Brazil. Though, I have spent most of my life where I currently live João Pessoa, Brazil.\nI\u0026rsquo;m a Computer Engineering Undergrad student at the Federal University of Paraíba, and a Machine Learning Researcher at ArIA - Artificial Intelligence Applications Laboratory, where I\u0026rsquo;ve been studying Artificial Intelligence for the past months.\nI\u0026rsquo;m also the Co-founder and Vice president of TAIL (Technology and Artificial Intelligence League). We\u0026rsquo;re a research group, made by students, focused on spread knowledge on our region. Our primary goal is to construct a strong community of Artificial Intelligence developers.\nOutside of computer science, I enjoy reading Sci-fi, traveling, playing Dungeons \u0026amp; Dragons, and caffeine :)\n","date":1607299200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607299200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hey, I\u0026rsquo;m João Pedro.\nI\u0026rsquo;m from São Paulo, Brazil. Though, I have spent most of my life where I currently live João Pessoa, Brazil.\nI\u0026rsquo;m a Computer Engineering Undergrad student at the Federal University of Paraíba, and a Machine Learning Researcher at ArIA - Artificial Intelligence Applications Laboratory, where I\u0026rsquo;ve been studying Artificial Intelligence for the past months.","tags":null,"title":"João Pedro Vasconcelos Teixeira","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://jpvt.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"b7398d6d21c673b2e24582d72f73a0a1","permalink":"https://jpvt.github.io/project/gandido/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/project/gandido/","section":"project","summary":"In this project we trained a CycleGan to reproduce Candido's Portinari style into photos of places and people.","tags":["Deep Learning","GANs","Neural Style Transfer"],"title":"GANdido Portinari","type":"project"},{"authors":["João Pedro Vasconcelos Teixeira"],"categories":["Computer Vision","Digital Image Processing","Deep Learning"],"content":"Table of Contents  Description Requirements Median Filtering AutoEncoder References  1. Description Many image processing applications make use of digitalized textual data. However, the presence of any type of noise can create difficulties in post-processing information, such as on OCR detection. To improve the information manipulation on such data, a previous image processing step is required.\nIn light of this idea, a set of text paragraphs containing plain English language was collected. Different font styles, size, and background noise level were arranged to simulate the a variety of scenarios.\nThe objective of this article is to evaluate the possible image processing methods that could fix the text samples. Note that the samples have a different type of background noise and present a set of text fonts. Therefore, the candidate should provide a flexible algorithm that can correctly detect what is text characters and background noise, offering a clean version of each text paragraph as result.\n2. Requirements Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:\n!pip install numpy !pip install scipy !pip install pillow !pip install opencv-python !pip install matplotlib !pip install tensorflow !pip install scikit-learn  ######################################################## from os import listdir from os.path import isfile, join ######################################################## import numpy as np from scipy import signal from PIL import Image import cv2 import matplotlib.pyplot as plt ######################################################## import tensorflow as tf from tensorflow import keras from tensorflow.keras import Sequential, Model, layers from sklearn.model_selection import train_test_split ######################################################## # Comment the following code if you don't have tensorflow-gpu installed in your enviroment gpu = len(tf.config.list_physical_devices('GPU'))\u0026gt;0 print(\u0026quot;GPU is\u0026quot;, \u0026quot;available\u0026quot; if gpu else \u0026quot;NOT AVAILABLE\u0026quot;) physical_devices = tf.config.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(physical_devices[0], True)  GPU is available  3. Median Filtering A simple way to solve this problem is using classic digital image processing techniques since it will not be necessary to train any machine learning algorithms, which usually require more data, time, and better hardware. So, as a first answer, I will apply the median filter to get the background of the image, then I will subtract it from the original image.\nclass Denoiser: \u0026quot;\u0026quot;\u0026quot; Class that creates and object capable of colect dirty images and partially clean some of its noise \u0026quot;\u0026quot;\u0026quot; def __init__(self): self.input_image = None self.output_image = None def load_image(self, path): # Load an image from the path and return it as a numpy array return np.asarray(Image.open(path))/255.0 def save_image(self, dest_path, image): # Take some image array(image) and save it in the destination(dest_path) img_arr = np.asarray(image*255.0, dtype=np.uint8) Image.fromarray(img_arr).save(dest_path) def denoiser_pipeline(self, image): # Collect background background = signal.medfilt2d(image, 11) # Select anything that is darker than the background foreground_mask = image \u0026lt; background - 0.1 # Return black for anything that is darker than the background or white otherwise output_image = np.where(foreground_mask, 0.0, 1.0) return output_image def clean(self, image_path, dest_path): # Load the input image self.input_image = self.load_image(image_path) # Process the image and load it in the output self.output_image = self.denoiser_pipeline(self.input_image) # Saves the output self.save_image(dest_path, self.output_image) def show(self): # Show the last image filtered in a kernel in_img = np.asarray(self.input_image*255.0, dtype = np.uint8) out_img = np.asarray(self.output_image*255.0, dtype = np.uint8) fig , axs = plt.subplots(1,2, figsize = (16,16)) axs[0].imshow(Image.fromarray(in_img), cmap = 'gray') axs[1].imshow(Image.fromarray(out_img), cmap = 'gray') axs[0].axis('off') axs[1].axis('off') plt.show()  input_datapath = 'noisy_data/' output_datapath = 'output_median/' files = [f for f in listdir(input_datapath) if isfile(join(input_datapath, f))] denoiser = Denoiser() for f in files: denoiser.clean(input_datapath + f, output_datapath + f)  denoiser.show()  Analyzing the processed images is possible to see that the algorithm works relatively well, considering its simplicity. It is possible to use it in scenarios without many resources to use more sophisticated and modern solutions.\nThe results are available in the output_median directory.\n4. AutoEncoder A more sophisticated answer to the problem is using the \u0026ldquo;magic\u0026rdquo; of Deep Learning. For this, I will use an Autoencoder network, which is composed of an encoder and a decoder. The encoder compresses the data to a smaller representation. The decoder reconstructs this representation to obtain an output close to the input. During the process, the autoencoder learns the most important features that make up the data.\nAutoencoders can reduce image noise by providing noisy data as an input and outputting the same data without noise. Then, the autoencoder can learn how to filter similar data efficiently.\nBut how is it possible to apply this to the proposed problem?\nAs stated in the test description, cleaning document images is a well-documented problem. Then it is possible to find databases that provide images with background noise and their respective clean version with ease. After looking for a database that fits the task, I trained an autoencoder using its dirty data as input and its clean images as output. So, I used the network to filter the dirty inputs presented by the challenge.\n# Simple image process to standardize our data def process_image(path): img = cv2.imread(path) img = np.asarray(img, dtype=\u0026quot;float32\u0026quot;) img = cv2.resize(img, (540, 420)) img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) img = img/255.0 img = np.reshape(img, (420, 540, 1)) return img  # Loading the training images noisy_path = 'train/' clean_path = 'train_cleaned/' test_path = 'test/' noisy_dir = listdir(noisy_path) x_paths = [noisy_path+x for x in noisy_dir] clean_dir = listdir(clean_path) y_paths = [clean_path+y for y in clean_dir] size = (420,540) X = [] y = [] for image in x_paths: img = process_image(image) X.append(img) for label in y_paths: img = process_image(label) y.append(img) X = np.array(X) y = np.array(y) fig, axs = plt.subplots(1,2, figsize = (16,16)) axs[0].set_title('Dirty Image') axs[0].imshow(X[0][:,:,0], cmap = 'gray') axs[0].axis('off') axs[1].set_title('Clean Image') axs[1].imshow(y[0][:,:,0], cmap = 'gray') axs[1].axis('off') plt.show() print(\u0026quot;Size of X : \u0026quot;, X.shape) print(\u0026quot;Size of Y : \u0026quot;, y.shape)  Size of X : (144, 420, 540, 1) Size of Y : (144, 420, 540, 1)  For the construction of the autoencoder, I will use:\n Convolutional layers to extract the significant features of the images; Max-pooling for data compression; Up-sampling for restoring the data dimension; Batch normalization to reduce the difference in the distribution of activations between the layers.  def autoencoder(): input_layer = layers.Input(shape=(420,540,1), name= 'Image_Input') # Encoder x = layers.Conv2D(32, (3,3), activation = 'relu', padding = 'same', name = 'Convolutional_Layer_1')(input_layer) x = layers.Conv2D(64, (3,3), activation = 'relu', padding = 'same', name = 'Convolutional_Layer_2')(x) x = layers.BatchNormalization()(x) x = layers.MaxPooling2D((2,2), padding = 'same', name='Pooling_Layer_3')(x) # Decoder x = layers.Conv2D(64, (3,3), activation = 'relu', padding = 'same', name = 'Convolutional_Layer_5')(x) x = layers.Conv2D(32, (3,3), activation = 'relu', padding = 'same', name = 'Convolutional_Layer_6')(x) x = layers.BatchNormalization()(x) x = layers.UpSampling2D((2,2), name = 'UpSampling_Layer_3')(x) output_layer = layers.Conv2D(1, (3,3), activation = 'sigmoid', padding = 'same', name = 'Convolutional_Layer_7')(x) # Model autoencoder = Model(inputs = [input_layer], outputs = [output_layer]) autoencoder.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['mae']) return autoencoder  ae = autoencoder() ae.summary()  Model: \u0026quot;model\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Image_Input (InputLayer) [(None, 420, 540, 1)] 0 _________________________________________________________________ Convolutional_Layer_1 (Conv2 (None, 420, 540, 32) 320 _________________________________________________________________ Convolutional_Layer_2 (Conv2 (None, 420, 540, 64) 18496 _________________________________________________________________ batch_normalization (BatchNo (None, 420, 540, 64) 256 _________________________________________________________________ Pooling_Layer_3 (MaxPooling2 (None, 210, 270, 64) 0 _________________________________________________________________ Convolutional_Layer_5 (Conv2 (None, 210, 270, 64) 36928 _________________________________________________________________ Convolutional_Layer_6 (Conv2 (None, 210, 270, 32) 18464 _________________________________________________________________ batch_normalization_1 (Batch (None, 210, 270, 32) 128 _________________________________________________________________ UpSampling_Layer_3 (UpSampli (None, 420, 540, 32) 0 _________________________________________________________________ Convolutional_Layer_7 (Conv2 (None, 420, 540, 1) 289 ================================================================= Total params: 74,881 Trainable params: 74,689 Non-trainable params: 192 _________________________________________________________________  keras.utils.plot_model(ae, \u0026quot;assets/autoencoder.png\u0026quot;, show_shapes=True)  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0) print(\u0026quot;Total number of training samples: \u0026quot;, X_train.shape) print(\u0026quot;Total number of validation samples: \u0026quot;, X_valid.shape)  Total number of training samples: (129, 420, 540, 1) Total number of validation samples: (15, 420, 540, 1)  callback = keras.callbacks.EarlyStopping(monitor='loss', patience=20) hist = ae.fit(X_train, y_train, epochs=50, batch_size=2, validation_data=(X_valid, y_valid), callbacks = [callback])  epoch_loss = hist.history['loss'] epoch_val_loss = hist.history['val_loss'] epoch_mae = hist.history['mae'] epoch_val_mae = hist.history['val_mae'] plt.figure(figsize=(20,6)) plt.subplot(1,2,1) plt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss') plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss') plt.title('Evolution of loss on train \u0026amp; validation datasets over epochs') plt.legend(loc='best') plt.subplot(1,2,2) plt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE') plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE') plt.title('Evolution of MAE on train \u0026amp; validation datasets over epochs') plt.legend(loc='best') plt.show()  The neural network seems to have learned to solve the problem well for the data sample provided so far, as you can see in the graphs.\nHowever, this does not mean that the network has learned well for images it has never seen, different noises, rotated texts.\nThen we will test the images on the noisy data provided by the test to check if the algorithm used was efficient.\ntest_path = 'noisy_data/' test_dir = listdir(test_path) test_paths = [test_path+x for x in test_dir] X_test = [] for image in test_paths: img = process_image(image) X_test.append(img) X_test = np.array(X_test) print(X_test.shape)  (144, 420, 540, 1)  Y_test = ae.predict(X_test, batch_size=4)  plt.figure(figsize=(15,25)) for i in range(0,8,2): plt.subplot(4,2,i+1) plt.xticks([]) plt.yticks([]) plt.imshow(X_test[i][:,:,0], cmap='gray') plt.title('Noisy image: {}'.format(test_paths[i])) plt.subplot(4,2,i+2) plt.xticks([]) plt.yticks([]) plt.imshow(Y_test[i][:,:,0], cmap='gray') plt.title('Denoised by autoencoder: {}'.format(test_paths[i])) plt.show()  As you can see in the images above, the approached strategy was very efficient in removing the noise from the data provided. It seems that the model has learned to distinguish the background from the images well, even with different types of noise and rotated texts.\n5. References  Denoising Dirty Documents Kaggle Competition Denoising with AutoEncoders - T81-558: Applications of Deep Neural Networks Denoise images using Autoencoder[TF, Keras]   ","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"ee84bc393cd4be9d39aa0990f73fe67b","permalink":"https://jpvt.github.io/post/documentcleanup/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/documentcleanup/","section":"post","summary":"Many image processing applications make use of digitalized textual data. However, the presence of any type of noise can create difficulties in post-processing information, such as on OCR detection. To improve the information manipulation on such data, a previous image processing step is required. The objective of this article is to evaluate the possible image processing methods that could fix the text samples.","tags":["Digital Image Processing","Denoising Images","Image","Deep Learning","Autoenconder","Computer Vision","Tensorflow"],"title":"Denoising Documents with Computer Vision and Digital Image Processing","type":"post"},{"authors":["João Pedro Vasconcelos Teixeira"],"categories":["Natural Language Processing","Classification","Machine Learning","Data Analysis","Deep Learning"],"content":"Table of Contents  Description Requirements Data Analysis and Feature Engineering Data preprocessing Classic ML LSTM References  1. Description The SMS Ham-Spam detection dataset is a set of SMS tagged messages that have been collected for SMS Spam research. It contains a set of 5,574 SMS messages in English, considering both train and test data. The tagging standard was defined as ham (legitimate) or spam.\nThe train and test files are formatted using the standard of one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\nham What you doing?how are you? ham Ok lar... Joking wif u oni... ham dun say so early hor... U c already then say... ham MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H* ham Siva is in hostel aha:-. ham Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor. spam FreeMsg: Txt: CALL to No: 86888 \u0026amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop spam Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B spam URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU  Note: messages are not chronologically sorted.  For evaluation purposes, the test dataset does not prosent the categories (ham, spam). Therefore, the train data is the full source of information for this test.\nThe goal of the this test is to achieve a model that can correctly manage the incoming messages on SMS format (ham or spam). Considering a real scenario, assume that a regular person does not want to see a spam message. However, they accepts if a normal message (ham) is sometimes allocated at the spam box.\n2. Requirements Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:\n!pip install numpy !pip install matplotlib !pip install tensorflow !pip install scikit-learn !pip install nltk !pip install transformers !pip install seaborn !pip install xgboost  ################################################################################################# import re from collections import Counter import time ################################################################################################# import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt ################################################################################################# from sklearn import feature_extraction from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from transformers import TFTrainer, TFTrainingArguments from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.svm import SVC from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier from xgboost import XGBClassifier ################################################################################################# import nltk from nltk import WordNetLemmatizer from nltk.corpus import stopwords from nltk.tokenize import word_tokenize ################################################################################################# import tensorflow as tf from tensorflow import keras from tensorflow.keras import Sequential, Model, layers from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing import sequence from tensorflow.keras.callbacks import EarlyStopping ################################################################################################# gpu = len(tf.config.list_physical_devices('GPU'))\u0026gt;0 if gpu: print(\u0026quot;GPU is\u0026quot;, \u0026quot;available\u0026quot;) physical_devices = tf.config.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(physical_devices[0], True) else: print(\u0026quot;NOT AVAILABLE\u0026quot;)  nltk.download(\u0026quot;punkt\u0026quot;) nltk.download(\u0026quot;stopwords\u0026quot;) nltk.download(\u0026quot;wordnet\u0026quot;)  3. Data Analysis and Feature Engineering TRAIN_PATH = 'TrainingSet/sms-hamspam-train.csv' df = pd.read_csv(TRAIN_PATH,names = ['class','text'], delimiter = '\\t') df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  class text     0 ham Go until jurong point, crazy.. Available only ...   1 spam SMSSERVICES. for yourinclusive text credits, p...   2 spam 25p 4 alfie Moon's Children in need song on ur...   3 spam U have a secret admirer. REVEAL who thinks U R...   4 spam Dear Voucher Holder, To claim this weeks offer...   ... ... ...   4720 spam This is the 2nd time we have tried 2 contact u...   4721 ham Will ü b going to esplanade fr home?   4722 ham Pity, * was in mood for that. So...any other s...   4723 ham The guy did some bitching but I acted like i'd...   4724 ham Rofl. Its true to its name    4725 rows × 2 columns\n fig, axs = plt.subplots(3, 3, figsize = (16,16)) # Class Distribution class_value_counts = df['class'].value_counts(sort = True) # Plot axs[0,0].set_title(\u0026quot;Class Distribution\u0026quot;) axs[0,0].pie(class_value_counts, labels = class_value_counts.index, autopct = \u0026quot;%1.0f%%\u0026quot;) axs[0,0].axis('off') # Word Frequency most_common_ham = pd.DataFrame.from_dict( Counter(\u0026quot; \u0026quot;.join(df.loc[df[\u0026quot;class\u0026quot;]== \u0026quot;ham\u0026quot;][\u0026quot;text\u0026quot;]).split()).most_common(10) ) most_common_ham = most_common_ham.rename(columns={0: \u0026quot;word_in_ham\u0026quot;, 1 : \u0026quot;frequency\u0026quot;}) most_common_spam = pd.DataFrame.from_dict( Counter(\u0026quot; \u0026quot;.join(df.loc[df[\u0026quot;class\u0026quot;]== \u0026quot;spam\u0026quot;][\u0026quot;text\u0026quot;]).split()).most_common(10) ) most_common_spam = most_common_spam.rename(columns={0: \u0026quot;word_in_spam\u0026quot;, 1 : \u0026quot;frequency\u0026quot;}) axs[0,1].set_title(\u0026quot;Word Frequency in Ham SMS\u0026quot;) axs[0,1].bar(most_common_ham[\u0026quot;word_in_ham\u0026quot;], most_common_ham[\u0026quot;frequency\u0026quot;]) axs[0,1].set_xticks(np.arange(len(most_common_ham[\u0026quot;word_in_ham\u0026quot;]))) axs[0,1].set_xticklabels(most_common_ham[\u0026quot;word_in_ham\u0026quot;]) axs[0,1].set_xlabel(\u0026quot;Words\u0026quot;) axs[0,1].set_ylabel(\u0026quot;Frequency\u0026quot;) axs[0,2].set_title(\u0026quot;Word Frequency in Spam SMS\u0026quot;) axs[0,2].bar(most_common_spam[\u0026quot;word_in_spam\u0026quot;], most_common_spam[\u0026quot;frequency\u0026quot;], color = 'orange') axs[0,2].set_xticks(np.arange(len(most_common_spam[\u0026quot;word_in_spam\u0026quot;]))) axs[0,1].set_xticklabels(most_common_spam[\u0026quot;word_in_spam\u0026quot;]) axs[0,2].set_xlabel(\u0026quot;Words\u0026quot;) axs[0,2].set_ylabel(\u0026quot;Frequency\u0026quot;) # Length df[\u0026quot;message_len\u0026quot;] = df[\u0026quot;text\u0026quot;].apply(len) sns.kdeplot( df.loc[df['class'] == \u0026quot;ham\u0026quot;, \u0026quot;message_len\u0026quot;], shade=True, label=\u0026quot;Ham\u0026quot;, clip=(-50, 250), ax = axs[1,0] ) sns.kdeplot(df.loc[df['class'] == \u0026quot;spam\u0026quot;, \u0026quot;message_len\u0026quot;], shade=True, label=\u0026quot;Spam\u0026quot;,ax = axs[1,0]) axs[1,0].set( xlabel=\u0026quot;Length\u0026quot;, ylabel=\u0026quot;Density\u0026quot;, title=\u0026quot;Length of SMS.\u0026quot;, ) axs[1,0].legend(loc=\u0026quot;upper right\u0026quot;) # Number of Words df[\u0026quot;nwords\u0026quot;] = df[\u0026quot;text\u0026quot;].apply(lambda s: len(re.findall(r\u0026quot;\\w+\u0026quot;, s))) sns.kdeplot( df.loc[df['class'] == \u0026quot;ham\u0026quot;, \u0026quot;nwords\u0026quot;], shade=True, label=\u0026quot;Ham\u0026quot;, clip=(-10, 50), ax = axs[1,1] ) sns.kdeplot(df.loc[df['class'] == \u0026quot;spam\u0026quot;, \u0026quot;nwords\u0026quot;], shade=True, label=\u0026quot;Spam\u0026quot;, ax = axs[1,1]) axs[1,1].set( xlabel=\u0026quot;Words\u0026quot;, ylabel=\u0026quot;Density\u0026quot;, title=\u0026quot;Number of Words in SMS.\u0026quot;, ) axs[1,1].legend(loc=\u0026quot;upper right\u0026quot;) # Number of Uppercased Words df[\u0026quot;nupperwords\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda s: len(re.findall(r\u0026quot;\\b[A-Z][A-Z]+\\b\u0026quot;, s)) ) sns.kdeplot( df.loc[df['class'] == \u0026quot;ham\u0026quot;, \u0026quot;nupperwords\u0026quot;], shade=True, label=\u0026quot;Ham\u0026quot;, clip=(0, 35), ax = axs[1,2] ) sns.kdeplot(df.loc[df['class'] == \u0026quot;spam\u0026quot;, \u0026quot;nupperwords\u0026quot;], shade=True, label=\u0026quot;Spam\u0026quot;, ax = axs[1,2]) axs[1,2].set( xlabel=\u0026quot;Uppercased Words\u0026quot;, ylabel=\u0026quot;Density\u0026quot;, title=\u0026quot;Number of Uppercased Words.\u0026quot;, ) axs[1,2].legend(loc=\u0026quot;upper right\u0026quot;) # Number of Uppercased Characters df[\u0026quot;nupperchars\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda s: sum(1 for c in s if c.isupper()) ) sns.scatterplot(x=\u0026quot;message_len\u0026quot;, y=\u0026quot;nupperchars\u0026quot;, hue=\u0026quot;class\u0026quot;, data=df, ax = axs[2,0]) axs[2,0].set( xlabel=\u0026quot;Characters\u0026quot;, ylabel=\u0026quot;Uppercase Characters\u0026quot;, title=\u0026quot;Number of Uppercased Characters in SMS.\u0026quot;, ) axs[2,0].legend(loc=\u0026quot;upper right\u0026quot;) # Contains free or win df[\u0026quot;is_free_or_win\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda s: int(\u0026quot;free\u0026quot; in s.lower() or \u0026quot;win\u0026quot; in s.lower()) ) grouped_data = ( df.groupby(\u0026quot;class\u0026quot;)[\u0026quot;is_free_or_win\u0026quot;] .value_counts(normalize=True) .rename(\u0026quot;Percentage of Group\u0026quot;) .reset_index() ) axs[2,1].set_title(\u0026quot;Distribution of FREE/WIN Words Between Spam and Ham\u0026quot;) sns.barplot( x=\u0026quot;class\u0026quot;, y=\u0026quot;Percentage of Group\u0026quot;, hue=\u0026quot;is_free_or_win\u0026quot;, data=grouped_data, ax = axs[2,1] ) # Contains url df[\u0026quot;is_url\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda s: 1 if re.search( r\u0026quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.\u0026amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\u0026quot;, s, ) else 0 ) grouped_data = ( df.groupby(\u0026quot;class\u0026quot;)[\u0026quot;is_url\u0026quot;] .value_counts(normalize=True) .rename(\u0026quot;Percentage of Group\u0026quot;) .reset_index() ) axs[2,2].set_title(\u0026quot;Distribution of URL Between Spam and Ham\u0026quot;) sns.barplot( x=\u0026quot;class\u0026quot;, y=\u0026quot;Percentage of Group\u0026quot;, hue=\u0026quot;is_url\u0026quot;, data=grouped_data, ax = axs[2,2] ) plt.savefig('assets/data_analysis.jpg') plt.show()  4. Data Preprocessing df[\u0026quot;text\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda row: re.sub(r\u0026quot;[^a-zA-Z]+\u0026quot;, \u0026quot; \u0026quot;, row) ) df[\u0026quot;text\u0026quot;].head()  0 Go until jurong point crazy Available only in ... 1 SMSSERVICES for yourinclusive text credits pls... 2 p alfie Moon s Children in need song on ur mo... 3 U have a secret admirer REVEAL who thinks U R ... 4 Dear Voucher Holder To claim this weeks offer ... Name: text, dtype: object  df[\u0026quot;text\u0026quot;] = df[\u0026quot;text\u0026quot;].apply(lambda row: word_tokenize(row)) df[\u0026quot;text\u0026quot;].head()  0 [Go, until, jurong, point, crazy, Available, o... 1 [SMSSERVICES, for, yourinclusive, text, credit... 2 [p, alfie, Moon, s, Children, in, need, song, ... 3 [U, have, a, secret, admirer, REVEAL, who, thi... 4 [Dear, Voucher, Holder, To, claim, this, weeks... Name: text, dtype: object  df[\u0026quot;text\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda row: [ token for token in row if token not in set(stopwords.words(\u0026quot;english\u0026quot;)) ] ) df[\u0026quot;text\u0026quot;].head()  0 [Go, jurong, point, crazy, Available, bugis, n... 1 [SMSSERVICES, yourinclusive, text, credits, pl... 2 [p, alfie, Moon, Children, need, song, ur, mob... 3 [U, secret, admirer, REVEAL, thinks, U, R, So,... 4 [Dear, Voucher, Holder, To, claim, weeks, offe... Name: text, dtype: object  df[\u0026quot;text\u0026quot;] = df[\u0026quot;text\u0026quot;].apply( lambda row: \u0026quot; \u0026quot;.join([WordNetLemmatizer().lemmatize(word) for word in row]) ) df[\u0026quot;text\u0026quot;].head()  0 Go jurong point crazy Available bugis n great ... 1 SMSSERVICES yourinclusive text credit pls goto... 2 p alfie Moon Children need song ur mob Tell ur... 3 U secret admirer REVEAL think U R So special C... 4 Dear Voucher Holder To claim week offer PC ple... Name: text, dtype: object  df.loc[df['class'] == \u0026quot;ham\u0026quot;, \u0026quot;class\u0026quot;] = 0 df.loc[df['class'] == \u0026quot;spam\u0026quot;, \u0026quot;class\u0026quot;] = 1  df.head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  class text message_len nwords nupperwords nupperchars is_free_or_win is_url     0 0 Go jurong point crazy Available bugis n great ... 111 20 0 3 0 0   1 1 SMSSERVICES yourinclusive text credit pls goto... 156 24 3 24 0 0   2 1 p alfie Moon Children need song ur mob Tell ur... 161 32 0 7 0 0   3 1 U secret admirer REVEAL think U R So special C... 147 28 3 24 0 0   4 1 Dear Voucher Holder To claim week offer PC ple... 152 31 2 13 0 1   5 1 Do want anytime network min text NEW VIDEO pho... 149 28 2 9 0 0   6 1 We tried contact offer New Video Phone anytime... 155 28 2 15 0 0   7 1 Last chance claim ur worth discount voucher Te... 160 31 2 16 0 0   8 1 Urgent call landline Your complimentary Ibiza ... 153 28 3 18 0 0   9 1 Today Offer Claim ur worth discount voucher Te... 158 29 1 14 0 0     df['class'] = df['class'].astype(np.uint8)  df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  class message_len nwords nupperwords nupperchars is_free_or_win is_url     count 4725.000000 4725.000000 4725.000000 4725.000000 4725.000000 4725.000000 4725.000000   mean 0.132275 80.161270 16.180106 0.646138 5.679788 0.071111 0.003175   std 0.338825 60.559271 11.947438 2.551516 11.932286 0.257038 0.056260   min 0.000000 2.000000 0.000000 0.000000 0.000000 0.000000 0.000000   25% 0.000000 36.000000 8.000000 0.000000 1.000000 0.000000 0.000000   50% 0.000000 61.000000 12.000000 0.000000 2.000000 0.000000 0.000000   75% 0.000000 121.000000 24.000000 0.000000 4.000000 0.000000 0.000000   max 1.000000 910.000000 190.000000 32.000000 129.000000 1.000000 1.000000     df.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 4725 entries, 0 to 4724 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 class 4725 non-null uint8 1 text 4725 non-null object 2 message_len 4725 non-null int64 3 nwords 4725 non-null int64 4 nupperwords 4725 non-null int64 5 nupperchars 4725 non-null int64 6 is_free_or_win 4725 non-null int64 7 is_url 4725 non-null int64 dtypes: int64(6), object(1), uint8(1) memory usage: 263.1+ KB  X_train, X_val, y_train, y_val = train_test_split( df[\u0026quot;text\u0026quot;], df[\u0026quot;class\u0026quot;], test_size=0.1, random_state = 0 ) print(f\u0026quot;Training data: {len(X_train)} (90%)\u0026quot;) print(f\u0026quot;Testing data: {len(X_val)} (10%)\u0026quot;)  Training data: 4252 (90%) Testing data: 473 (10%)  5. Classic ML knn = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, KNeighborsClassifier()) ] ), { \u0026quot;clf__n_neighbors\u0026quot;: (3,5,15,25,45,55), } ) mnbayes = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, MultinomialNB()), ] ), { \u0026quot;clf__alpha\u0026quot;: (0.1, 1e-2, 1e-3), \u0026quot;clf__fit_prior\u0026quot;: (True, False), }, ) svc = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, SVC(gamma=\u0026quot;auto\u0026quot;, C=1000)), ] ), { } ) ada = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, AdaBoostClassifier()), ] ), { \u0026quot;clf__n_estimators\u0026quot;: [100,200], \u0026quot;clf__learning_rate\u0026quot;: [0.001, 0.01, 0.1, 0.2, 0.5] } ) rf = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, RandomForestClassifier()), ] ), { \u0026quot;clf__criterion\u0026quot; : [\u0026quot;gini\u0026quot;, \u0026quot;entropy\u0026quot;], \u0026quot;clf__max_depth\u0026quot; : [None, 1,3,5,10], \u0026quot;clf__min_samples_split\u0026quot;: [5,10], \u0026quot;clf__min_samples_leaf\u0026quot;: [5,10], \u0026quot;clf__n_estimators\u0026quot;: [100, 150, 200] } ) xgb = GridSearchCV( Pipeline( [ (\u0026quot;BagOfWords\u0026quot;, CountVectorizer()), (\u0026quot;Tfidf\u0026quot;, TfidfTransformer()), (\u0026quot;clf\u0026quot;, XGBClassifier()), ] ), { 'clf__max_depth': [3, 4, 5], \u0026quot;clf__n_estimators\u0026quot;: [200, 500, 600] } ) models = {\u0026quot;K-Nearest Neighbors\u0026quot;: knn, \u0026quot;Multinomial Naive Bayes\u0026quot;: mnbayes, \u0026quot;Support Vector Machine\u0026quot;: svc, \u0026quot;AdaBoost\u0026quot;: ada, \u0026quot;Random Forest\u0026quot;: rf, \u0026quot;XGBoost\u0026quot;: xgb } results = [] for model in models: print(\u0026quot;\\n\u0026quot;,model) models[model].fit(X= X_train, y = y_train) preds = models[model].predict(X_val) plt.figure(figsize=(10,4)) heatmap = sns.heatmap( data = pd.DataFrame(confusion_matrix(y_val, preds)), annot = True, fmt = \u0026quot;d\u0026quot;, cmap=sns.color_palette(\u0026quot;Blues\u0026quot;, 50), ) heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14) heatmap.yaxis.set_ticklabels( heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14 ) plt.title(\u0026quot;Confusion Matrix\u0026quot;) plt.ylabel(\u0026quot;Ground Truth\u0026quot;) plt.xlabel(\u0026quot;Prediction\u0026quot;) plt.show() precision = precision_score(y_val, preds) recall = recall_score(y_val, preds) acc = accuracy_score(y_val, preds) print(f\u0026quot;Precision: {precision * 100:.3f}%\u0026quot;) print(f\u0026quot; Recall: {recall * 100 :.3f}%\u0026quot;) print(f\u0026quot; Accuracy: {acc * 100:.3f}%\u0026quot;) results.append([model, precision, recall, acc])   K-Nearest Neighbors  Precision: 100.000% Recall: 64.615% Accuracy: 95.137% Multinomial Naive Bayes  Precision: 98.387% Recall: 93.846% Accuracy: 98.943% Support Vector Machine  Precision: 98.305% Recall: 89.231% Accuracy: 98.309% AdaBoost  Precision: 98.361% Recall: 92.308% Accuracy: 98.732% Random Forest  Precision: 98.148% Recall: 81.538% Accuracy: 97.252% XGBoost  Precision: 96.610% Recall: 87.692% Accuracy: 97.886%  6. LSTM max_words = 1000 max_len = 150 tok = Tokenizer(num_words=max_words) tok.fit_on_texts(X_train) sequences = tok.texts_to_sequences(X_train) sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)  sequences_val = tok.texts_to_sequences(X_val) sequences_matrix_val = sequence.pad_sequences(sequences_val, maxlen = max_len)  def build_rnn(): input_layer = layers.Input(shape = [max_len], name='Input_Layer') x = layers.Embedding(max_words, 50, input_length = max_len, name = \u0026quot;Embedding_Layer\u0026quot;)(input_layer) x = layers.LSTM(64, activation = 'tanh', name = \u0026quot;LSTM_Layer\u0026quot;)(x) x = layers.Dense(256, activation = 'relu', name = 'Dense_Layer_1')(x) #x = layers.Dropout(0.5)(x) output_layer = layers.Dense(1, activation='sigmoid',name = 'Output_Layer')(x) rnn = Model(inputs=input_layer, outputs=output_layer) return rnn  model = build_rnn() model.summary() keras.utils.plot_model(model, \u0026quot;assets/simple_rnn.png\u0026quot;, show_shapes=True)  Model: \u0026quot;model\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= Input_Layer (InputLayer) [(None, 150)] 0 _________________________________________________________________ Embedding_Layer (Embedding) (None, 150, 50) 50000 _________________________________________________________________ LSTM_Layer (LSTM) (None, 64) 29440 _________________________________________________________________ Dense_Layer_1 (Dense) (None, 256) 16640 _________________________________________________________________ Output_Layer (Dense) (None, 1) 257 ================================================================= Total params: 96,337 Trainable params: 96,337 Non-trainable params: 0 _________________________________________________________________  model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics =['accuracy',keras.metrics.Precision(),keras.metrics.Recall()])  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001)  hist = model.fit(sequences_matrix, y_train, batch_size = 128, epochs = 10, validation_data=(sequences_matrix_val, y_val))  epoch_loss = hist.history['loss'] epoch_val_loss = hist.history['val_loss'] epoch_acc = hist.history['accuracy'] epoch_val_acc = hist.history['val_accuracy'] epoch_precision = hist.history['precision'] epoch_val_precision = hist.history['val_precision'] epoch_recall = hist.history['recall'] epoch_val_recall = hist.history['val_recall'] fig, axs = plt.subplots(2,2, figsize = (16,16)) plt.figure(figsize=(20,6)) axs[0,0].plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss') axs[0,0].plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss') axs[0,0].set_title('Evolution of loss on train \u0026amp; validation datasets over epochs') axs[0,0].legend(loc='best') axs[0,1].plot(range(0,len(epoch_acc)), epoch_acc, 'b-', linewidth=2, label='Train accuracy') axs[0,1].plot(range(0,len(epoch_val_acc)), epoch_val_acc, 'r-', linewidth=2,label='Val accuracy') axs[0,1].set_title('Evolution of accuracy on train \u0026amp; validation datasets over epochs') axs[0,1].legend(loc='best') axs[1,0].plot(range(0,len(epoch_precision)), epoch_precision, 'b-', linewidth=2, label='Train precision') axs[1,0].plot(range(0,len(epoch_val_precision)), epoch_val_precision, 'r-', linewidth=2, label='Val precision') axs[1,0].set_title('Evolution of precision on train \u0026amp; validation datasets over epochs') axs[1,0].legend(loc='best') axs[1,1].plot(range(0,len(epoch_recall)), epoch_recall, 'b-', linewidth=2, label='Train recall') axs[1,1].plot(range(0,len(epoch_val_recall)), epoch_val_recall, 'r-', linewidth=2,label='Val recall') axs[1,1].set_title('Evolution of recall on train \u0026amp; validation datasets over epochs') axs[1,1].legend(loc='best') plt.show()  \u0026lt;Figure size 1440x432 with 0 Axes\u0026gt;  preds = model.predict(sequences_matrix_val) preds = np.uint8(np.round(preds.T))  plt.figure(figsize=(10,4)) heatmap = sns.heatmap( data = pd.DataFrame(confusion_matrix(y_val.values, preds[0])), annot = True, fmt = \u0026quot;d\u0026quot;, cmap=sns.color_palette(\u0026quot;Blues\u0026quot;, 50), ) heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14) heatmap.yaxis.set_ticklabels( heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14 ) plt.title(\u0026quot;Confusion Matrix\u0026quot;) plt.ylabel(\u0026quot;Ground Truth\u0026quot;) plt.xlabel(\u0026quot;Prediction\u0026quot;) plt.show() precision = precision_score(y_val.values, preds[0]) recall = recall_score(y_val.values, preds[0]) acc = accuracy_score(y_val.values, preds[0]) print(f\u0026quot;Precision: {precision * 100:.3f}%\u0026quot;) print(f\u0026quot; Recall: {recall * 100 :.3f}%\u0026quot;) print(f\u0026quot; Accuracy: {acc * 100:.3f}%\u0026quot;) results.append(['Simple RNN', precision, recall, acc])  Precision: 96.875% Recall: 95.385% Accuracy: 98.943%  model.save(f'saved_models/my_model_{time.time()}.h5')  df_results = pd.DataFrame(results, columns=['Algorithm', 'Precision', 'Recall', 'Accuracy'])  df_results   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Algorithm Precision Recall Accuracy     0 K-Nearest Neighbors 1.000000 0.646154 0.951374   1 Multinomial Naive Bayes 0.983871 0.938462 0.989429   2 Support Vector Machine 0.983051 0.892308 0.983087   3 AdaBoost 0.983607 0.923077 0.987315   4 Random Forest 0.981481 0.815385 0.972516   5 XGBoost 0.966102 0.876923 0.978858   6 Simple RNN 0.968750 0.953846 0.989429     Evaluating the test TEST_PATH = 'TestSet/sms-hamspam-test.csv' df_test = pd.read_csv(TEST_PATH,names = ['text']) df_test   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  text     0 I know that my friend already told that.   1 It took Mr owl 3 licks   2 Dunno y u ask me.   3 K.k:)advance happy pongal.   4 I know but you need to get hotel now. I just g...   ... ...   842 Booked ticket for pongal?   843 Yes :)it completely in out of form:)clark also...   844 Yeah sure   845 He is there. You call and meet him   846 I see a cup of coffee animation    847 rows × 1 columns\n def apply_data_preprocessing(dataframe, column): df = dataframe.copy() df[column] = df[column].apply( lambda row: re.sub(r\u0026quot;[^a-zA-Z]+\u0026quot;, \u0026quot; \u0026quot;, row) ) df[column] = df[column].apply(lambda row: word_tokenize(row)) df[column] = df[column].apply( lambda row: [ token for token in row if token not in set(stopwords.words(\u0026quot;english\u0026quot;)) ] ) df[column] = df[column].apply( lambda row: \u0026quot; \u0026quot;.join([WordNetLemmatizer().lemmatize(word) for word in row]) ) X = df[column].values max_words = 1000 max_len = 150 tok = Tokenizer(num_words=max_words) tok.fit_on_texts(X) sequences = tok.texts_to_sequences(X) sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len) return sequences_matrix  def classify(text_csv, column, model_path): df_test = pd.read_csv(text_csv,names = [column]) sequences_matrix = apply_data_preprocessing(df_test, column) model = tf.keras.models.load_model(model_path) preds = model.predict(sequences_matrix) preds = np.uint8(np.round(preds.T)) preds = preds[0] df_test['class'] = preds return df_test  7. References  SMS Spam Detection - Kaggle notebooks  ","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"87cc26c0b5e2ae9b2fcc7425a8b618a5","permalink":"https://jpvt.github.io/project/smsspamdetection/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/project/smsspamdetection/","section":"project","summary":"The goal of the this project is to achieve a model that can correctly manage the incoming messages on SMS format (`ham` or `spam`).","tags":["Natural Language Processing","Classification","Machine Learning","Data Analysis","LSTM","Recurrent Neural Networks","Deep Learning"],"title":"SMS Spam Detection with Machine Learning","type":"project"},{"authors":["João Pedro Vasconcelos Teixeira"],"categories":["Data Analysis"],"content":"Metro Interstate Traffic Volume Analysis Introduction In this notebook, I will discuss the results of my second assignment of the class Introduction to Artificial Intelligence. My job was to predict the traffic volume on Metro Interstate with machine learning models. Finally, I will discuss my results and present some insights into the data.\nDataset The dataset used for this assignment is used by many people all over the world, mainly for learning purposes.\nAbout: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.\nThis dataset is available at: https://archive.ics.uci.edu/ml/machine-learning-databases/00492/\nImporting Packages # Data Preprocessing Packages import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.preprocessing import LabelEncoder, OneHotEncoder import re # Data Visualization Packages import matplotlib.pyplot as plt import seaborn as sns import pandas_profiling as pf from sklearn.ensemble import RandomForestRegressor # Models and Metrics from sklearn import model_selection, svm from sklearn.svm import SVR from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score from sklearn.model_selection import cross_val_score, cross_val_predict,GridSearchCV, StratifiedKFold, KFold, RandomizedSearchCV, train_test_split import xgboost as xgb  Getting the data: dataset = False if dataset: df = pd.read_csv(\u0026quot;metro.csv\u0026quot;) else: df = pd.read_csv(\u0026quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\u0026quot;) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume     0 None 288.28 0.0 0.0 40 Clouds scattered clouds 2012-10-02 09:00:00 5545   1 None 289.36 0.0 0.0 75 Clouds broken clouds 2012-10-02 10:00:00 4516   2 None 289.58 0.0 0.0 90 Clouds overcast clouds 2012-10-02 11:00:00 4767   3 None 290.13 0.0 0.0 90 Clouds overcast clouds 2012-10-02 12:00:00 5026   4 None 291.14 0.0 0.0 75 Clouds broken clouds 2012-10-02 13:00:00 4918     df.info() #Basic information about each column  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 48204 entries, 0 to 48203 Data columns (total 9 columns): holiday 48204 non-null object temp 48204 non-null float64 rain_1h 48204 non-null float64 snow_1h 48204 non-null float64 clouds_all 48204 non-null int64 weather_main 48204 non-null object weather_description 48204 non-null object date_time 48204 non-null object traffic_volume 48204 non-null int64 dtypes: float64(3), int64(2), object(4) memory usage: 3.3+ MB  Descrições do dataset, valores numéricos e categóricos df.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  temp rain_1h snow_1h clouds_all traffic_volume     count 48204.000000 48204.000000 48204.000000 48204.000000 48204.000000   mean 281.205870 0.334264 0.000222 49.362231 3259.818355   std 13.338232 44.789133 0.008168 39.015750 1986.860670   min 0.000000 0.000000 0.000000 0.000000 0.000000   25% 272.160000 0.000000 0.000000 1.000000 1193.000000   50% 282.450000 0.000000 0.000000 64.000000 3380.000000   75% 291.806000 0.000000 0.000000 90.000000 4933.000000   max 310.070000 9831.300000 0.510000 100.000000 7280.000000     df.describe(include= 'object')   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday weather_main weather_description date_time     count 48204 48204 48204 48204   unique 12 11 38 40575   top None Clouds sky is clear 2013-04-18 22:00:00   freq 48143 15164 11665 6     Exploratory Data Analysis: It is extremely important to know the dataset well since in machine learning is the diversity in the experience that will guarantee success in carrying out a given task. Statistics in this process is very useful, as it provides descriptive measures that demonstrate the main characteristics of the data we are dealing with. Additionally, searching for information about the data and the problem to which it is linked can be of great help and even essential to improve the achievement of the desired task.   About the data:  Holiday: Indicates if the date is a holiday and if it specifies the holiday, if not None. Temp: Indicates the temperature in Kelvin. rain_1h: Amount in mm of rain that occurred in the hour. snow_1h: Amount in mm of snow that occurred in the hour. clouds_all: Percentage of cloud cover. weather_main: Short textual description of the current weather. weather_description: Longer textual description of the current weather. date_time: Hour of the data collected in local CST time. traffic_volume: Hourly I-94 ATR 301 reported westbound traffic volume.  disponível em: https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume\nThe first observations to be made with the information we have so far are:\n There is no missing data, but that does not mean that there is no inconsistent data.  The date_time, a time stamp, is not defined as the pandas' timestamp. The way it was extracted will not bring us any information and that is a big problem. Since in most cities, traffic occurs at rush hour.  There are temperature records at absolute zero, clearly inconsistent data.  rain_1h and snow_1h have many zeros and their distribution is not very well defined, since in rare moments they have high records.  99.9% of Holiday data is None, and the other data is spread over multiple holidays  Holiday plt.figure(figsize = (16,6)) sns.countplot(y = df['holiday']) plt.show()  As normal days are much more frequent over the years of the dataset, then holidays are not visible in the Plot. To analyze them it is necessary to remove the normal days. Then I will categorize between Holiday and Non-Holiday\nh_df = df[df['holiday'] != 'None'] plt.figure(figsize = (16,6)) sns.countplot(y = h_df['holiday']) plt.show()  To solve this problem, I will recategorize all holidays as Holiday and leave them in the same category.\nTemperature plt.figure(figsize = (16,8)) sns.boxplot(df['temp']) plt.show()  As seen before, some Outliers are at absolute zero, probably due to some error in capturing the temperature. Then I will remove the Outliers so as not to affect my result.\nRain and Snow plt.figure(figsize = (16,6)) sns.distplot(df['rain_1h'], kde = True) plt.show() plt.figure(figsize = (16,6)) sns.distplot(df['snow_1h'], kde = True) plt.show()  As can be seen, the features have many zeros and the distribution is skewed.\nClouds plt.figure(figsize = (16,6)) sns.distplot(df['clouds_all'],) plt.show() df['clouds_all'].describe()  count 48204.000000 mean 49.362231 std 39.015750 min 0.000000 25% 1.000000 50% 64.000000 75% 90.000000 max 100.000000 Name: clouds_all, dtype: float64  It has many zeros, but does not appear to show any inconsistent data.\nWeather Main e Weather Description plt.figure(figsize = (16,6)) sns.countplot(y = df['weather_main']) plt.show() plt.figure(figsize = (16,8)) sns.countplot(y =df['weather_description']) plt.show() confusion_matrix = pd.crosstab(df['weather_main'], df['weather_description']) confusion_matrix.corr(method = 'spearman').style.background_gradient(cmap='coolwarm')  Since weather_description offers only an extension of the description of weather_main, it is probably not of interest to keep both for modeling. Since they carry correlated information, which can be seen in the matrix above.\nData Cleaning Now that the exploratory data analysis has been done, it will be necessary to clean the dataset, to guarantee the success of the model. The observations made at the EDA will be made below.\nHoliday Parsing def holiday(holiday): cat = 'None' if holiday != 'None': cat = 'Holiday' return cat df['holiday'] = df['holiday'].map(holiday) df['holiday'].unique()  array(['None', 'Holiday'], dtype=object)  Parsing do date_time def parse_timestamp(df, datetime): df[datetime] = pd.to_datetime(df[datetime]) df['Year'] = df[datetime].dt.year df['Month'] = df[datetime].dt.month df['Weekday'] = df[datetime].dt.weekday df['Hour'] = df[datetime].dt.hour def categorize_hour(hour): cat = 'None' if hour in [1,2,3,4,5]: cat = 'dawn' elif hour in [6,7,8,9,10,11,12]: cat = 'morning' elif hour in [13,14,15,16,17,18]: cat = 'afternoon' elif hour in [19,20,21,22,23,24]: cat = 'night' return cat  Before parsing the date_time i will remove possible duplicates.\ndf.drop_duplicates(inplace = True) df.duplicated().sum()  0  parse_timestamp(df, 'date_time') #df['Hour'] = df['Hour'].map(categorize_hour) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp rain_1h snow_1h clouds_all weather_main weather_description date_time traffic_volume Year Month Weekday Hour     0 None 288.28 0.0 0.0 40 Clouds scattered clouds 2012-10-02 09:00:00 5545 2012 10 1 9   1 None 289.36 0.0 0.0 75 Clouds broken clouds 2012-10-02 10:00:00 4516 2012 10 1 10   2 None 289.58 0.0 0.0 90 Clouds overcast clouds 2012-10-02 11:00:00 4767 2012 10 1 11   3 None 290.13 0.0 0.0 90 Clouds overcast clouds 2012-10-02 12:00:00 5026 2012 10 1 12   4 None 291.14 0.0 0.0 75 Clouds broken clouds 2012-10-02 13:00:00 4918 2012 10 1 13     df.set_index('date_time',inplace = True) df.sort_index()['traffic_volume'].rolling(4000).mean().plot(figsize = (16,8)) plt.xlabel('Date Time') plt.ylabel('Traffic Volume') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp rain_1h snow_1h clouds_all weather_main weather_description traffic_volume Year Month Weekday Hour   date_time                 2012-10-02 09:00:00 None 288.28 0.0 0.0 40 Clouds scattered clouds 5545 2012 10 1 9   2012-10-02 10:00:00 None 289.36 0.0 0.0 75 Clouds broken clouds 4516 2012 10 1 10   2012-10-02 11:00:00 None 289.58 0.0 0.0 90 Clouds overcast clouds 4767 2012 10 1 11   2012-10-02 12:00:00 None 290.13 0.0 0.0 90 Clouds overcast clouds 5026 2012 10 1 12   2012-10-02 13:00:00 None 291.14 0.0 0.0 75 Clouds broken clouds 4918 2012 10 1 13     Removing Weather_description I have chosen to remove weather_description due to redundancy when placing with weather_main\ndf.drop(columns = ['weather_description'], inplace = True) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp rain_1h snow_1h clouds_all weather_main traffic_volume Year Month Weekday Hour   date_time                2012-10-02 09:00:00 None 288.28 0.0 0.0 40 Clouds 5545 2012 10 1 9   2012-10-02 10:00:00 None 289.36 0.0 0.0 75 Clouds 4516 2012 10 1 10   2012-10-02 11:00:00 None 289.58 0.0 0.0 90 Clouds 4767 2012 10 1 11   2012-10-02 12:00:00 None 290.13 0.0 0.0 90 Clouds 5026 2012 10 1 12   2012-10-02 13:00:00 None 291.14 0.0 0.0 75 Clouds 4918 2012 10 1 13     Removendo rain_1h e snow_1h As the distribution of rain_1h and snow_1h is not good and the vast majority of values are at zero, I will remove them, since the weather information includes whether it is raining, snowing, or neither.\ndf.drop(columns = ['rain_1h','snow_1h'],inplace = True)  df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp clouds_all weather_main traffic_volume Year Month Weekday Hour   date_time              2012-10-02 09:00:00 None 288.28 40 Clouds 5545 2012 10 1 9   2012-10-02 10:00:00 None 289.36 75 Clouds 4516 2012 10 1 10   2012-10-02 11:00:00 None 289.58 90 Clouds 4767 2012 10 1 11   2012-10-02 12:00:00 None 290.13 90 Clouds 5026 2012 10 1 12   2012-10-02 13:00:00 None 291.14 75 Clouds 4918 2012 10 1 13     Feature Importance I will run a Random Forest in order to find the importance of each feature for the result of traffic_volume. However, it is necessary to transform categorical data first.\nSplitting Features and Traffic Volume feats = df.drop('traffic_volume',1).copy() label = df['traffic_volume'].copy() feats.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp clouds_all weather_main Year Month Weekday Hour   date_time             2012-10-02 09:00:00 None 288.28 40 Clouds 2012 10 1 9   2012-10-02 10:00:00 None 289.36 75 Clouds 2012 10 1 10   2012-10-02 11:00:00 None 289.58 90 Clouds 2012 10 1 11   2012-10-02 12:00:00 None 290.13 90 Clouds 2012 10 1 12   2012-10-02 13:00:00 None 291.14 75 Clouds 2012 10 1 13     Copy with Label Enconder of the features le = LabelEncoder() num_feats = feats.copy() for column in num_feats.columns: if num_feats[column].dtype == 'object': num_feats[column] = le.fit_transform(num_feats[column]) num_feats.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  holiday temp clouds_all weather_main Year Month Weekday Hour   date_time             2012-10-02 09:00:00 1 288.28 40 1 2012 10 1 9   2012-10-02 10:00:00 1 289.36 75 1 2012 10 1 10   2012-10-02 11:00:00 1 289.58 90 1 2012 10 1 11   2012-10-02 12:00:00 1 290.13 90 1 2012 10 1 12   2012-10-02 13:00:00 1 291.14 75 1 2012 10 1 13     Copy with One Hot Enconding of the features onehot_feats = feats.copy() onehot_feats[['Year','Month','Weekday']] = feats[['Year','Month','Weekday']].astype('category') onehot_feats = pd.get_dummies(onehot_feats) onehot_feats.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  temp clouds_all Hour holiday_Holiday holiday_None weather_main_Clear weather_main_Clouds weather_main_Drizzle weather_main_Fog weather_main_Haze ... Month_10 Month_11 Month_12 Weekday_0 Weekday_1 Weekday_2 Weekday_3 Weekday_4 Weekday_5 Weekday_6   date_time                          2012-10-02 09:00:00 288.28 40 9 0 1 0 1 0 0 0 ... 1 0 0 0 1 0 0 0 0 0   2012-10-02 10:00:00 289.36 75 10 0 1 0 1 0 0 0 ... 1 0 0 0 1 0 0 0 0 0   2012-10-02 11:00:00 289.58 90 11 0 1 0 1 0 0 0 ... 1 0 0 0 1 0 0 0 0 0   2012-10-02 12:00:00 290.13 90 12 0 1 0 1 0 0 0 ... 1 0 0 0 1 0 0 0 0 0   2012-10-02 13:00:00 291.14 75 13 0 1 0 1 0 0 0 ... 1 0 0 0 1 0 0 0 0 0    5 rows × 42 columns\n Feature Importance - Label Encoder model = RandomForestRegressor(random_state=1, max_depth=10) model.fit(num_feats,label) #Plot features = feats.columns importances = model.feature_importances_ indices = np.argsort(importances)[-5:] #Top 5 features plt.title('Feature Importances') plt.barh(range(len(indices)), importances[indices], color='b', align='center') plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel('Relative Importance') plt.show()  Feature Importance - One Hot Encoding model = RandomForestRegressor(random_state=1, max_depth=10) model.fit(onehot_feats,label) #Plot t features = onehot_feats.columns importances = model.feature_importances_ indices = np.argsort(importances)[-10:] #Top 10 features plt.title('Feature Importances') plt.barh(range(len(indices)), importances[indices], color='b', align='center') plt.yticks(range(len(indices)), [features[i] for i in indices]) plt.xlabel('Relative Importance') plt.show()  It is clear now that the hour feature is the one that most influences the traffic volume. This can be commonly observed in big cities where at rush hour the traffic volume is high.\nScaling Necessary to keep the numerical values in a range and consequently one feature does not stand out much more than the other when training the model.\nX = num_feats.values X = preprocessing.scale(X) y = label.values print(X) print(y)  [[ 0.03560205 0.53041592 -0.24004863 ... 1.02779521 -0.98947829 -0.34548099] [ 0.03560205 0.61138394 0.65704665 ... 1.02779521 -0.98947829 -0.20139502] [ 0.03560205 0.62787742 1.04151605 ... 1.02779521 -0.98947829 -0.05730906] ... [ 0.03560205 0.11433026 1.04151605 ... 0.73369913 1.50301966 1.38355059] [ 0.03560205 0.06634921 1.04151605 ... 0.73369913 1.50301966 1.52763656] [ 0.03560205 0.06859832 1.04151605 ... 0.73369913 1.50301966 1.67172252]] [5545 4516 4767 ... 2159 1450 954]  Train and Test Holdout(0.7; 0.3) The dataset will be randomly split between training and testing. For this task, we will divide 70% for training, while the rest will be used for testing.\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42) X_train.shape  (33730, 8)  Random Forest It is a supervised ML(Machine Learning) algorithm. Creates several decision trees and combines them to obtain a better prediction.   Random forest adds randomness to the model when creating trees. It searches for the best attribute in a random subset of the attribute. This creates diversity and generates better models. For this reason, it was possible to find the importance of attributes, performed previously.    Advantages:\n Can be used for classification and regression. Handles missing values well. It will hardly overfit the model. Handles large, high dimensional data sets very well (Main reason for choosing this method)    Disadvantages:\n The programmer does not have much control on the model, it generates black box.     Random Forest Example:\n  reg = RandomForestRegressor() reg = reg.fit(X_train,y_train) predict = reg.predict(X_test) accuracy = reg.score(X_test, y_test) mae = mean_absolute_error(y_test, predict) print(f\u0026quot;R2 Score: {round(accuracy * 100, 3)}%\u0026quot;) print(f\u0026quot;Mean Absolute Error: {round(mae, 4)}\u0026quot;) print(f'RMSE: {round(np.sqrt(mean_squared_error(y_pred=predict,y_true=y_test)),4)}')  R2 Score: 95.901% Mean Absolute Error: 221.4494 RMSE: 405.0598  XGBoost It is an implementation of the Gradient Boost supervised ML algorithm, famous for its speed and performance. Gradient Boost tries to predict a target variable, for this it combines a group of estimates from a set of simpler and \u0026lsquo;weaker\u0026rsquo; models and transforms them into a \u0026lsquo;stronger\u0026rsquo; model.   Gradient Boosting Example:\n  In this example, it can be seen that Gradient Boosting is a method in which new models are trained to resolve errors from previous models.\nxgb_model = xgb.XGBRegressor(objective=\u0026quot;reg:squarederror\u0026quot;, n_estimators=1000, random_state=42,n_jobs=-1) xgb_model.fit(X_train,y_train,early_stopping_rounds=10,eval_set=[(X_test, y_test)],verbose = 0) y_pred = xgb_model.predict(X_test) xgb_mae = mean_absolute_error(y_test, y_pred) print(f'R2 Score: {round(xgb_model.score(X_test,y_test)*100,2)}') print(f'RMSE: {np.sqrt(mean_squared_error(y_pred=y_pred,y_true=y_test))}') print(f\u0026quot;Mean Absolute Error: {round(xgb_mae, 4)}\u0026quot;)  R2 Score: 96.19 RMSE: 390.32545493877905 Mean Absolute Error: 232.6679  Cross-Validation In this method, the dataset is divided into K parts, while K-1 parts are used for training the remaining partition is used for testing until all the dataset is utilized. The validation of each forecast made is saved in a list and then the average of each metric will be made.\nkf = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)  Random Forest rmse = [] r2 = [] mae_l = [] for train_index, test_index in kf.split(X): reg_kf = RandomForestRegressor() reg_kf.fit(X[train_index],y[train_index]) y_pred = reg_kf.predict(X[test_index]) actuals = y[test_index] rmse.append(np.sqrt(mean_squared_error(actuals, y_pred))) r2.append(r2_score(actuals,y_pred)) mae_l.append(mean_absolute_error(actuals, y_pred)) avg_r2 = np.mean(r2) avg_rmse = np.mean(rmse) avg_mae = np.mean(mae_l) print(f'AVG R2 Score :{round(avg_r2,3)*100}%\\t Average RMSE:{avg_rmse}\\t Average MAE:{round(avg_mae, 4)}')  AVG R2 Score :95.7%\tAverage RMSE:409.69403930472544\tAverage MAE:217.4925  XGBoost rmse = [] r2 = [] mae_l = [] for train_index, test_index in kf.split(X): xgb_model = xgb.XGBRegressor(objective=\u0026quot;reg:squarederror\u0026quot;, n_estimators=1000, random_state=42,n_jobs=-1) xgb_model.fit(X[train_index],y[train_index],early_stopping_rounds=10,eval_set=[(X[test_index], y[test_index])], verbose = 0) predictions = xgb_model.predict(X[test_index]) actuals = y[test_index] rmse.append(np.sqrt(mean_squared_error(actuals, predictions))) r2.append(r2_score(actuals,predictions)) mae_l.append(mean_absolute_error(actuals, predictions)) avg_r2 = np.mean(r2) avg_rmse = np.mean(rmse) avg_mae = np.mean(mae_l) print(f'AVG R2 Score :{round(avg_r2,3)*100}%\\t Average RMSE:{avg_rmse}\\t Average MAE:{round(avg_mae, 4)}')  AVG R2 Score :96.1%\tAverage RMSE:390.5595082859465\tAverage MAE:229.8818  Results and Discussion: The metrics chosen for the evaluation of the models were:\n RMSE(Root Mean Squared Error), average of the difference between the actual and the predicted value. MAE(Mean Absolute Error), module mean of the difference between the actual and the predicted value. R²(R Squared), it is a measure that demonstrates how much the model explains the variance of the real value. It varies between zero and one, the closer to one the more adjusted to the sample is the model.   From the results presented for each model, it is noticeable that the forecasts fit well with the expected sample. The Holdout method showed better efficiency for the problem than the cross-validation, although the results are not so different. The XGBoost algorithm obtained forecasts with less error and more adjusted to the original sample.   Despite the satisfying result of the predictive models, they may not be efficient for certain applications, like a route recommendation system, where the value must be very precise. To solve this problem, the next works are expected to implement specific models for time-based problems, such as Recurrent Neural Networks (RNN) and ARIMA, since the features that are most relevant to the problem are related to time series. ","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"20576e455483ae8c4d8ccda2fb6bcf71","permalink":"https://jpvt.github.io/post/metro_traffic_volume/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/post/metro_traffic_volume/","section":"post","summary":"In this notebook, I will discuss the results of my second assignment of the class Introduction to Artificial Intelligence. My job was to predict the traffic volume on Metro Interstate with machine learning models. Finally, I will discuss my results and present some insights into the data.","tags":["Data Analysis","Regression","Random Forest","XGB","Machine Learning"],"title":"Metro Interstate Traffic Volume Analysis","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jpvt.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["João Pedro Vasconcelos Teixeira","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://jpvt.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://jpvt.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]