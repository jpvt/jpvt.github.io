<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing | João Pedro Vasconcelos</title>
    <link>https://jpvt.github.io/category/natural-language-processing/</link>
      <atom:link href="https://jpvt.github.io/category/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <description>Natural Language Processing</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jpvt.github.io/media/icon_hu6e0ba196f4e113be19f679f5e0d6caf1_39320_512x512_fill_lanczos_center_2.png</url>
      <title>Natural Language Processing</title>
      <link>https://jpvt.github.io/category/natural-language-processing/</link>
    </image>
    
    <item>
      <title>SMS Spam Detection with Machine Learning</title>
      <link>https://jpvt.github.io/project/smsspamdetection/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/project/smsspamdetection/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Requirements&lt;/li&gt;
&lt;li&gt;Data Analysis and Feature Engineering&lt;/li&gt;
&lt;li&gt;Data preprocessing&lt;/li&gt;
&lt;li&gt;Classic ML&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-description&#34;&gt;1. Description&lt;/h2&gt;
&lt;p&gt;The SMS Ham-Spam detection dataset is a set of SMS tagged messages that have been collected for SMS Spam research. It contains a set of 5,574 SMS messages in English, considering both train and test data. The tagging standard was defined as &lt;code&gt;ham&lt;/code&gt; (legitimate) or &lt;code&gt;spam&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; files are formatted using the standard of one message per line. Each line is composed by two columns: one with label (&lt;code&gt;ham&lt;/code&gt; or &lt;code&gt;spam&lt;/code&gt;) and other with the raw text. Here are some examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ham   What you doing?how are you?
ham   Ok lar... Joking wif u oni...
ham   dun say so early hor... U c already then say...
ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*
ham   Siva is in hostel aha:-.
ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.
spam   FreeMsg: Txt: CALL to No: 86888 &amp;amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop
spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B
spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Note: messages are not chronologically sorted.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For evaluation purposes, the &lt;code&gt;test&lt;/code&gt; dataset does not prosent the categories (&lt;code&gt;ham&lt;/code&gt;, &lt;code&gt;spam&lt;/code&gt;). Therefore, the &lt;code&gt;train&lt;/code&gt; data is the full source of information for this test.&lt;/p&gt;
&lt;p&gt;The goal of the this test is to achieve a model that can correctly manage the incoming messages on SMS format (&lt;code&gt;ham&lt;/code&gt; or &lt;code&gt;spam&lt;/code&gt;). Considering a real scenario, assume that a regular person does not want to see a &lt;code&gt;spam&lt;/code&gt; message. However, they accepts if a normal message (&lt;code&gt;ham&lt;/code&gt;) is sometimes allocated at the &lt;code&gt;spam&lt;/code&gt; box.&lt;/p&gt;
&lt;h2 id=&#34;2-requirements&#34;&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;!pip install numpy
!pip install matplotlib
!pip install tensorflow
!pip install scikit-learn
!pip install nltk
!pip install transformers
!pip install seaborn
!pip install xgboost
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#################################################################################################

import re
from collections import Counter
import time

#################################################################################################

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#################################################################################################

from sklearn import feature_extraction
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from transformers import TFTrainer, TFTrainingArguments
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from xgboost import XGBClassifier

#################################################################################################

import nltk
from nltk import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#################################################################################################

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model, layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import EarlyStopping

#################################################################################################

gpu = len(tf.config.list_physical_devices(&#39;GPU&#39;))&amp;gt;0

if gpu:
    print(&amp;quot;GPU is&amp;quot;, &amp;quot;available&amp;quot;)
    physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;)
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
else:
    print(&amp;quot;NOT AVAILABLE&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nltk.download(&amp;quot;punkt&amp;quot;)
nltk.download(&amp;quot;stopwords&amp;quot;)
nltk.download(&amp;quot;wordnet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-data-analysis-and-feature-engineering&#34;&gt;3. Data Analysis and Feature Engineering&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TRAIN_PATH = &#39;TrainingSet/sms-hamspam-train.csv&#39;

df = pd.read_csv(TRAIN_PATH,names = [&#39;class&#39;,&#39;text&#39;], delimiter = &#39;\t&#39;)

df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Go until jurong point, crazy.. Available only ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;SMSSERVICES. for yourinclusive text credits, p...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;25p 4 alfie Moon&#39;s Children in need song on ur...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;U have a secret admirer. REVEAL who thinks U R...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;Dear Voucher Holder, To claim this weeks offer...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4720&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;This is the 2nd time we have tried 2 contact u...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4721&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Will ü b going to esplanade fr home?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4722&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Pity, * was in mood for that. So...any other s...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4723&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;The guy did some bitching but I acted like i&#39;d...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4724&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Rofl. Its true to its name&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;4725 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axs = plt.subplots(3, 3, figsize =  (16,16))

# Class Distribution
class_value_counts = df[&#39;class&#39;].value_counts(sort = True)

# Plot
axs[0,0].set_title(&amp;quot;Class Distribution&amp;quot;)
axs[0,0].pie(class_value_counts, labels = class_value_counts.index, autopct = &amp;quot;%1.0f%%&amp;quot;)
axs[0,0].axis(&#39;off&#39;)


# Word Frequency
most_common_ham = pd.DataFrame.from_dict(
    
    Counter(&amp;quot; &amp;quot;.join(df.loc[df[&amp;quot;class&amp;quot;]== &amp;quot;ham&amp;quot;][&amp;quot;text&amp;quot;]).split()).most_common(10)
)

most_common_ham = most_common_ham.rename(columns={0: &amp;quot;word_in_ham&amp;quot;, 1 : &amp;quot;frequency&amp;quot;})


most_common_spam = pd.DataFrame.from_dict(
    
    Counter(&amp;quot; &amp;quot;.join(df.loc[df[&amp;quot;class&amp;quot;]== &amp;quot;spam&amp;quot;][&amp;quot;text&amp;quot;]).split()).most_common(10)
)

most_common_spam = most_common_spam.rename(columns={0: &amp;quot;word_in_spam&amp;quot;, 1 : &amp;quot;frequency&amp;quot;})

axs[0,1].set_title(&amp;quot;Word Frequency in Ham SMS&amp;quot;)
axs[0,1].bar(most_common_ham[&amp;quot;word_in_ham&amp;quot;], most_common_ham[&amp;quot;frequency&amp;quot;])
axs[0,1].set_xticks(np.arange(len(most_common_ham[&amp;quot;word_in_ham&amp;quot;])))
axs[0,1].set_xticklabels(most_common_ham[&amp;quot;word_in_ham&amp;quot;])
axs[0,1].set_xlabel(&amp;quot;Words&amp;quot;)
axs[0,1].set_ylabel(&amp;quot;Frequency&amp;quot;)

axs[0,2].set_title(&amp;quot;Word Frequency in Spam SMS&amp;quot;)
axs[0,2].bar(most_common_spam[&amp;quot;word_in_spam&amp;quot;], most_common_spam[&amp;quot;frequency&amp;quot;], color = &#39;orange&#39;)
axs[0,2].set_xticks(np.arange(len(most_common_spam[&amp;quot;word_in_spam&amp;quot;])))
axs[0,1].set_xticklabels(most_common_spam[&amp;quot;word_in_spam&amp;quot;])
axs[0,2].set_xlabel(&amp;quot;Words&amp;quot;)
axs[0,2].set_ylabel(&amp;quot;Frequency&amp;quot;)

# Length
df[&amp;quot;message_len&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(len)

sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;message_len&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(-50, 250), ax = axs[1,0]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;message_len&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;,ax = axs[1,0])
axs[1,0].set(
    xlabel=&amp;quot;Length&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Length of SMS.&amp;quot;,
)
axs[1,0].legend(loc=&amp;quot;upper right&amp;quot;)


# Number of Words
df[&amp;quot;nwords&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(lambda s: len(re.findall(r&amp;quot;\w+&amp;quot;, s)))

sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;nwords&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(-10, 50), ax = axs[1,1]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;nwords&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;, ax = axs[1,1])

axs[1,1].set(
    xlabel=&amp;quot;Words&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Number of Words in SMS.&amp;quot;,
)
axs[1,1].legend(loc=&amp;quot;upper right&amp;quot;)


# Number of Uppercased Words
df[&amp;quot;nupperwords&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: len(re.findall(r&amp;quot;\b[A-Z][A-Z]+\b&amp;quot;, s))
)
sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;nupperwords&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(0, 35), ax = axs[1,2]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;nupperwords&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;, ax = axs[1,2])
axs[1,2].set(
    xlabel=&amp;quot;Uppercased Words&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Number of Uppercased Words.&amp;quot;,
)
axs[1,2].legend(loc=&amp;quot;upper right&amp;quot;)

# Number of Uppercased Characters
df[&amp;quot;nupperchars&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: sum(1 for c in s if c.isupper())
)

sns.scatterplot(x=&amp;quot;message_len&amp;quot;, y=&amp;quot;nupperchars&amp;quot;, hue=&amp;quot;class&amp;quot;, data=df, ax = axs[2,0])
axs[2,0].set(
    xlabel=&amp;quot;Characters&amp;quot;,
    ylabel=&amp;quot;Uppercase Characters&amp;quot;,
    title=&amp;quot;Number of Uppercased Characters in SMS.&amp;quot;,
)
axs[2,0].legend(loc=&amp;quot;upper right&amp;quot;)


# Contains free or win
df[&amp;quot;is_free_or_win&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: int(&amp;quot;free&amp;quot; in s.lower() or &amp;quot;win&amp;quot; in s.lower())
)


grouped_data = (
    df.groupby(&amp;quot;class&amp;quot;)[&amp;quot;is_free_or_win&amp;quot;]
    .value_counts(normalize=True)
    .rename(&amp;quot;Percentage of Group&amp;quot;)
    .reset_index()
)

axs[2,1].set_title(&amp;quot;Distribution of FREE/WIN Words Between Spam and Ham&amp;quot;)

sns.barplot(
    x=&amp;quot;class&amp;quot;,
    y=&amp;quot;Percentage of Group&amp;quot;,
    hue=&amp;quot;is_free_or_win&amp;quot;,
    data=grouped_data, ax = axs[2,1]
)


# Contains url
df[&amp;quot;is_url&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: 1
    if re.search(
        r&amp;quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&amp;quot;,
        s,
    )
    else 0
)


grouped_data = (
    df.groupby(&amp;quot;class&amp;quot;)[&amp;quot;is_url&amp;quot;]
    .value_counts(normalize=True)
    .rename(&amp;quot;Percentage of Group&amp;quot;)
    .reset_index()
)

axs[2,2].set_title(&amp;quot;Distribution of URL Between Spam and Ham&amp;quot;)

sns.barplot(
    x=&amp;quot;class&amp;quot;,
    y=&amp;quot;Percentage of Group&amp;quot;,
    hue=&amp;quot;is_url&amp;quot;,
    data=grouped_data, ax = axs[2,2]
)

plt.savefig(&#39;assets/data_analysis.jpg&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-data-preprocessing&#34;&gt;4. Data Preprocessing&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: re.sub(r&amp;quot;[^a-zA-Z]+&amp;quot;, &amp;quot; &amp;quot;, row)  
)

df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    Go until jurong point crazy Available only in ...
1    SMSSERVICES for yourinclusive text credits pls...
2     p alfie Moon s Children in need song on ur mo...
3    U have a secret admirer REVEAL who thinks U R ...
4    Dear Voucher Holder To claim this weeks offer ...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(lambda row: word_tokenize(row))
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    [Go, until, jurong, point, crazy, Available, o...
1    [SMSSERVICES, for, yourinclusive, text, credit...
2    [p, alfie, Moon, s, Children, in, need, song, ...
3    [U, have, a, secret, admirer, REVEAL, who, thi...
4    [Dear, Voucher, Holder, To, claim, this, weeks...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: [
        token for token in row if token not in set(stopwords.words(&amp;quot;english&amp;quot;))
    ]
)
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    [Go, jurong, point, crazy, Available, bugis, n...
1    [SMSSERVICES, yourinclusive, text, credits, pl...
2    [p, alfie, Moon, Children, need, song, ur, mob...
3    [U, secret, admirer, REVEAL, thinks, U, R, So,...
4    [Dear, Voucher, Holder, To, claim, weeks, offe...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: &amp;quot; &amp;quot;.join([WordNetLemmatizer().lemmatize(word) for word in row])
)
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    Go jurong point crazy Available bugis n great ...
1    SMSSERVICES yourinclusive text credit pls goto...
2    p alfie Moon Children need song ur mob Tell ur...
3    U secret admirer REVEAL think U R So special C...
4    Dear Voucher Holder To claim week offer PC ple...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;class&amp;quot;] = 0
df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;class&amp;quot;] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;message_len&lt;/th&gt;
      &lt;th&gt;nwords&lt;/th&gt;
      &lt;th&gt;nupperwords&lt;/th&gt;
      &lt;th&gt;nupperchars&lt;/th&gt;
      &lt;th&gt;is_free_or_win&lt;/th&gt;
      &lt;th&gt;is_url&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Go jurong point crazy Available bugis n great ...&lt;/td&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SMSSERVICES yourinclusive text credit pls goto...&lt;/td&gt;
      &lt;td&gt;156&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;p alfie Moon Children need song ur mob Tell ur...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;U secret admirer REVEAL think U R So special C...&lt;/td&gt;
      &lt;td&gt;147&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Dear Voucher Holder To claim week offer PC ple...&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Do want anytime network min text NEW VIDEO pho...&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;We tried contact offer New Video Phone anytime...&lt;/td&gt;
      &lt;td&gt;155&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Last chance claim ur worth discount voucher Te...&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Urgent call landline Your complimentary Ibiza ...&lt;/td&gt;
      &lt;td&gt;153&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Today Offer Claim ur worth discount voucher Te...&lt;/td&gt;
      &lt;td&gt;158&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;class&#39;] = df[&#39;class&#39;].astype(np.uint8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;message_len&lt;/th&gt;
      &lt;th&gt;nwords&lt;/th&gt;
      &lt;th&gt;nupperwords&lt;/th&gt;
      &lt;th&gt;nupperchars&lt;/th&gt;
      &lt;th&gt;is_free_or_win&lt;/th&gt;
      &lt;th&gt;is_url&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;0.132275&lt;/td&gt;
      &lt;td&gt;80.161270&lt;/td&gt;
      &lt;td&gt;16.180106&lt;/td&gt;
      &lt;td&gt;0.646138&lt;/td&gt;
      &lt;td&gt;5.679788&lt;/td&gt;
      &lt;td&gt;0.071111&lt;/td&gt;
      &lt;td&gt;0.003175&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;0.338825&lt;/td&gt;
      &lt;td&gt;60.559271&lt;/td&gt;
      &lt;td&gt;11.947438&lt;/td&gt;
      &lt;td&gt;2.551516&lt;/td&gt;
      &lt;td&gt;11.932286&lt;/td&gt;
      &lt;td&gt;0.257038&lt;/td&gt;
      &lt;td&gt;0.056260&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;36.000000&lt;/td&gt;
      &lt;td&gt;8.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;61.000000&lt;/td&gt;
      &lt;td&gt;12.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;121.000000&lt;/td&gt;
      &lt;td&gt;24.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;910.000000&lt;/td&gt;
      &lt;td&gt;190.000000&lt;/td&gt;
      &lt;td&gt;32.000000&lt;/td&gt;
      &lt;td&gt;129.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 4725 entries, 0 to 4724
Data columns (total 8 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   class           4725 non-null   uint8 
 1   text            4725 non-null   object
 2   message_len     4725 non-null   int64 
 3   nwords          4725 non-null   int64 
 4   nupperwords     4725 non-null   int64 
 5   nupperchars     4725 non-null   int64 
 6   is_free_or_win  4725 non-null   int64 
 7   is_url          4725 non-null   int64 
dtypes: int64(6), object(1), uint8(1)
memory usage: 263.1+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_val, y_train, y_val = train_test_split(
    df[&amp;quot;text&amp;quot;], df[&amp;quot;class&amp;quot;], test_size=0.1, random_state = 0
)



print(f&amp;quot;Training data: {len(X_train)} (90%)&amp;quot;)
print(f&amp;quot;Testing data: {len(X_val)} (10%)&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training data: 4252 (90%)
Testing data: 473 (10%)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;5-classic-ml&#34;&gt;5. Classic ML&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

knn = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, KNeighborsClassifier())
        ]
    ),
    {
        &amp;quot;clf__n_neighbors&amp;quot;: (3,5,15,25,45,55),
    }

)

mnbayes = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, MultinomialNB()),
        ]
    ),
    {
        &amp;quot;clf__alpha&amp;quot;: (0.1, 1e-2, 1e-3),
        &amp;quot;clf__fit_prior&amp;quot;: (True, False),
    },
)

svc = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, SVC(gamma=&amp;quot;auto&amp;quot;, C=1000)),
        ]
    ),
    {
        
    }
)


ada = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, AdaBoostClassifier()),
        ]
    ),
    {
        &amp;quot;clf__n_estimators&amp;quot;: [100,200],
        &amp;quot;clf__learning_rate&amp;quot;: [0.001, 0.01, 0.1, 0.2, 0.5]
    }
)

rf = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, RandomForestClassifier()),
        ]
    ),
    {
        &amp;quot;clf__criterion&amp;quot; : [&amp;quot;gini&amp;quot;, &amp;quot;entropy&amp;quot;],
        &amp;quot;clf__max_depth&amp;quot; :   [None, 1,3,5,10],
        &amp;quot;clf__min_samples_split&amp;quot;: [5,10],
        &amp;quot;clf__min_samples_leaf&amp;quot;: [5,10],
        &amp;quot;clf__n_estimators&amp;quot;: [100, 150, 200]
    }
)

xgb = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, XGBClassifier()),
        ]
    ),
    {
        &#39;clf__max_depth&#39;: [3, 4, 5],
        &amp;quot;clf__n_estimators&amp;quot;: [200, 500, 600]
    }
)


models = {&amp;quot;K-Nearest Neighbors&amp;quot;: knn,
          &amp;quot;Multinomial Naive Bayes&amp;quot;: mnbayes,
          &amp;quot;Support Vector Machine&amp;quot;: svc,
          &amp;quot;AdaBoost&amp;quot;: ada,
          &amp;quot;Random Forest&amp;quot;: rf,
          &amp;quot;XGBoost&amp;quot;: xgb
         }

results = []

for model in models:
    
    print(&amp;quot;\n&amp;quot;,model)
    
    models[model].fit(X= X_train, y = y_train)
    preds = models[model].predict(X_val)
    
    plt.figure(figsize=(10,4))

    heatmap = sns.heatmap(
        data = pd.DataFrame(confusion_matrix(y_val, preds)),
        annot = True,
        fmt = &amp;quot;d&amp;quot;,
        cmap=sns.color_palette(&amp;quot;Blues&amp;quot;, 50),
    )

    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)
    heatmap.yaxis.set_ticklabels(
        heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14
    )

    plt.title(&amp;quot;Confusion Matrix&amp;quot;)
    plt.ylabel(&amp;quot;Ground Truth&amp;quot;)
    plt.xlabel(&amp;quot;Prediction&amp;quot;)
    
    plt.show()
    
    precision = precision_score(y_val, preds)
    recall = recall_score(y_val, preds)
    acc = accuracy_score(y_val, preds)

    print(f&amp;quot;Precision: {precision * 100:.3f}%&amp;quot;)
    print(f&amp;quot;   Recall: {recall * 100 :.3f}%&amp;quot;)
    print(f&amp;quot; Accuracy: {acc * 100:.3f}%&amp;quot;)
    
    results.append([model, precision, recall, acc])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; K-Nearest Neighbors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 100.000%
   Recall: 64.615%
 Accuracy: 95.137%

 Multinomial Naive Bayes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.387%
   Recall: 93.846%
 Accuracy: 98.943%

 Support Vector Machine
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.305%
   Recall: 89.231%
 Accuracy: 98.309%

 AdaBoost
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_7.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.361%
   Recall: 92.308%
 Accuracy: 98.732%

 Random Forest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_9.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.148%
   Recall: 81.538%
 Accuracy: 97.252%

 XGBoost
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_11.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 96.610%
   Recall: 87.692%
 Accuracy: 97.886%
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;6-lstm&#34;&gt;6. LSTM&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_words = 1000
max_len = 150
tok = Tokenizer(num_words=max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sequences_val = tok.texts_to_sequences(X_val)
sequences_matrix_val = sequence.pad_sequences(sequences_val, maxlen = max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_rnn():
    
    input_layer = layers.Input(shape = [max_len], name=&#39;Input_Layer&#39;)
    
    x = layers.Embedding(max_words, 50, input_length = max_len, name = &amp;quot;Embedding_Layer&amp;quot;)(input_layer)
    x = layers.LSTM(64, activation = &#39;tanh&#39;, name = &amp;quot;LSTM_Layer&amp;quot;)(x)
    x = layers.Dense(256, activation = &#39;relu&#39;, name = &#39;Dense_Layer_1&#39;)(x)
    #x = layers.Dropout(0.5)(x)
    output_layer = layers.Dense(1, activation=&#39;sigmoid&#39;,name = &#39;Output_Layer&#39;)(x)
    
    rnn = Model(inputs=input_layer, outputs=output_layer)
    
    return rnn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = build_rnn()
model.summary()
keras.utils.plot_model(model, &amp;quot;assets/simple_rnn.png&amp;quot;, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input_Layer (InputLayer)     [(None, 150)]             0         
_________________________________________________________________
Embedding_Layer (Embedding)  (None, 150, 50)           50000     
_________________________________________________________________
LSTM_Layer (LSTM)            (None, 64)                29440     
_________________________________________________________________
Dense_Layer_1 (Dense)        (None, 256)               16640     
_________________________________________________________________
Output_Layer (Dense)         (None, 1)                 257       
=================================================================
Total params: 96,337
Trainable params: 96,337
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_30_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(loss = &#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;,
              metrics =[&#39;accuracy&#39;,keras.metrics.Precision(),keras.metrics.Recall()])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;early_stopping = keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0.0001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hist = model.fit(sequences_matrix, y_train, batch_size = 128, epochs = 10,
                 validation_data=(sequences_matrix_val, y_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_loss = hist.history[&#39;loss&#39;]
epoch_val_loss = hist.history[&#39;val_loss&#39;]

epoch_acc = hist.history[&#39;accuracy&#39;]
epoch_val_acc = hist.history[&#39;val_accuracy&#39;]

epoch_precision = hist.history[&#39;precision&#39;]
epoch_val_precision = hist.history[&#39;val_precision&#39;]

epoch_recall = hist.history[&#39;recall&#39;]
epoch_val_recall = hist.history[&#39;val_recall&#39;]


fig, axs = plt.subplots(2,2, figsize = (16,16))

plt.figure(figsize=(20,6))

axs[0,0].plot(range(0,len(epoch_loss)), epoch_loss, &#39;b-&#39;, linewidth=2, label=&#39;Train Loss&#39;)
axs[0,0].plot(range(0,len(epoch_val_loss)), epoch_val_loss, &#39;r-&#39;, linewidth=2, label=&#39;Val Loss&#39;)
axs[0,0].set_title(&#39;Evolution of loss on train &amp;amp; validation datasets over epochs&#39;)
axs[0,0].legend(loc=&#39;best&#39;)


axs[0,1].plot(range(0,len(epoch_acc)), epoch_acc, &#39;b-&#39;, linewidth=2, label=&#39;Train accuracy&#39;)
axs[0,1].plot(range(0,len(epoch_val_acc)), epoch_val_acc, &#39;r-&#39;, linewidth=2,label=&#39;Val accuracy&#39;)
axs[0,1].set_title(&#39;Evolution of accuracy on train &amp;amp; validation datasets over epochs&#39;)
axs[0,1].legend(loc=&#39;best&#39;)


axs[1,0].plot(range(0,len(epoch_precision)), epoch_precision, &#39;b-&#39;, linewidth=2, label=&#39;Train precision&#39;)
axs[1,0].plot(range(0,len(epoch_val_precision)), epoch_val_precision, &#39;r-&#39;, linewidth=2, label=&#39;Val precision&#39;)
axs[1,0].set_title(&#39;Evolution of precision on train &amp;amp; validation datasets over epochs&#39;)
axs[1,0].legend(loc=&#39;best&#39;)


axs[1,1].plot(range(0,len(epoch_recall)), epoch_recall, &#39;b-&#39;, linewidth=2, label=&#39;Train recall&#39;)
axs[1,1].plot(range(0,len(epoch_val_recall)), epoch_val_recall, &#39;r-&#39;, linewidth=2,label=&#39;Val recall&#39;)
axs[1,1].set_title(&#39;Evolution of recall on train &amp;amp; validation datasets over epochs&#39;)
axs[1,1].legend(loc=&#39;best&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1440x432 with 0 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;preds = model.predict(sequences_matrix_val)
preds = np.uint8(np.round(preds.T))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10,4))

heatmap = sns.heatmap(
    data = pd.DataFrame(confusion_matrix(y_val.values, preds[0])),
    annot = True,
    fmt = &amp;quot;d&amp;quot;,
    cmap=sns.color_palette(&amp;quot;Blues&amp;quot;, 50),
)

heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)
heatmap.yaxis.set_ticklabels(
    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14
)

plt.title(&amp;quot;Confusion Matrix&amp;quot;)
plt.ylabel(&amp;quot;Ground Truth&amp;quot;)
plt.xlabel(&amp;quot;Prediction&amp;quot;)

plt.show()

precision = precision_score(y_val.values, preds[0])
recall = recall_score(y_val.values, preds[0])
acc = accuracy_score(y_val.values, preds[0])

print(f&amp;quot;Precision: {precision * 100:.3f}%&amp;quot;)
print(f&amp;quot;   Recall: {recall * 100 :.3f}%&amp;quot;)
print(f&amp;quot; Accuracy: {acc * 100:.3f}%&amp;quot;)

results.append([&#39;Simple RNN&#39;, precision, recall, acc])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 96.875%
   Recall: 95.385%
 Accuracy: 98.943%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.save(f&#39;saved_models/my_model_{time.time()}.h5&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results = pd.DataFrame(results, columns=[&#39;Algorithm&#39;, &#39;Precision&#39;, &#39;Recall&#39;, &#39;Accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Algorithm&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;K-Nearest Neighbors&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.646154&lt;/td&gt;
      &lt;td&gt;0.951374&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Multinomial Naive Bayes&lt;/td&gt;
      &lt;td&gt;0.983871&lt;/td&gt;
      &lt;td&gt;0.938462&lt;/td&gt;
      &lt;td&gt;0.989429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Support Vector Machine&lt;/td&gt;
      &lt;td&gt;0.983051&lt;/td&gt;
      &lt;td&gt;0.892308&lt;/td&gt;
      &lt;td&gt;0.983087&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AdaBoost&lt;/td&gt;
      &lt;td&gt;0.983607&lt;/td&gt;
      &lt;td&gt;0.923077&lt;/td&gt;
      &lt;td&gt;0.987315&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;0.981481&lt;/td&gt;
      &lt;td&gt;0.815385&lt;/td&gt;
      &lt;td&gt;0.972516&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;XGBoost&lt;/td&gt;
      &lt;td&gt;0.966102&lt;/td&gt;
      &lt;td&gt;0.876923&lt;/td&gt;
      &lt;td&gt;0.978858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Simple RNN&lt;/td&gt;
      &lt;td&gt;0.968750&lt;/td&gt;
      &lt;td&gt;0.953846&lt;/td&gt;
      &lt;td&gt;0.989429&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;evaluating-the-test&#34;&gt;Evaluating the test&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TEST_PATH = &#39;TestSet/sms-hamspam-test.csv&#39;

df_test = pd.read_csv(TEST_PATH,names = [&#39;text&#39;])
df_test
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;I know that my friend already told that.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;It took Mr owl 3 licks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Dunno y u ask me.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;K.k:)advance happy pongal.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;I know but you need to get hotel now. I just g...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;842&lt;/th&gt;
      &lt;td&gt;Booked ticket for pongal?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;843&lt;/th&gt;
      &lt;td&gt;Yes :)it completely in out of form:)clark also...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;844&lt;/th&gt;
      &lt;td&gt;Yeah sure&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;845&lt;/th&gt;
      &lt;td&gt;He is there. You call and meet him&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;846&lt;/th&gt;
      &lt;td&gt;I see a cup of coffee animation&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;847 rows × 1 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def apply_data_preprocessing(dataframe, column):
    
    df = dataframe.copy()
    
    df[column] = df[column].apply(
        lambda row: re.sub(r&amp;quot;[^a-zA-Z]+&amp;quot;, &amp;quot; &amp;quot;, row)  
    )
    
    df[column] = df[column].apply(lambda row: word_tokenize(row))
    
    df[column] = df[column].apply(
        lambda row: [
            token for token in row if token not in set(stopwords.words(&amp;quot;english&amp;quot;))
        ]
    )
    
    df[column] = df[column].apply(
        lambda row: &amp;quot; &amp;quot;.join([WordNetLemmatizer().lemmatize(word) for word in row])
    )
    
    X = df[column].values
    
    max_words = 1000
    max_len = 150
    tok = Tokenizer(num_words=max_words)
    tok.fit_on_texts(X)
    sequences = tok.texts_to_sequences(X)
    sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
    
    return sequences_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classify(text_csv, column, model_path):
    
    df_test = pd.read_csv(text_csv,names = [column])
    
    sequences_matrix = apply_data_preprocessing(df_test, column)
    
    model = tf.keras.models.load_model(model_path)
    
    preds = model.predict(sequences_matrix)
    preds = np.uint8(np.round(preds.T))
    preds = preds[0]
    
    df_test[&#39;class&#39;] = preds
    
    return df_test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;7-references&#34;&gt;7. References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/uciml/sms-spam-collection-dataset/notebooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMS Spam Detection - Kaggle notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
