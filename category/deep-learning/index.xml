<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Jo√£o Pedro Vasconcelos</title>
    <link>https://jpvt.github.io/category/deep-learning/</link>
      <atom:link href="https://jpvt.github.io/category/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jpvt.github.io/media/icon_hu6e0ba196f4e113be19f679f5e0d6caf1_39320_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://jpvt.github.io/category/deep-learning/</link>
    </image>
    
    <item>
      <title>Denoising Documents with Computer Vision and Digital Image Processing</title>
      <link>https://jpvt.github.io/post/documentcleanup/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/post/documentcleanup/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Requirements&lt;/li&gt;
&lt;li&gt;Median Filtering&lt;/li&gt;
&lt;li&gt;AutoEncoder&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-description&#34;&gt;1. Description&lt;/h2&gt;
&lt;p&gt;Many image processing applications make use of digitalized textual data. However, the presence of any type of noise can create difficulties in post-processing information, such as on OCR detection. To improve the information manipulation on such data, a previous image processing step is required.&lt;/p&gt;
&lt;p&gt;In light of this idea, a set of text paragraphs containing plain English language was collected. Different font styles, size, and background noise level were arranged to simulate the a variety of scenarios.&lt;/p&gt;
&lt;p&gt;The objective of this article is to evaluate the possible image processing methods that could fix the text samples. Note that the samples have a different type of background noise and present a set of text fonts. Therefore, the candidate should provide a flexible algorithm that can correctly detect what is text characters and background noise, offering a clean version of each text paragraph as result.&lt;/p&gt;
&lt;h2 id=&#34;2-requirements&#34;&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;!pip install numpy
!pip install scipy
!pip install pillow
!pip install opencv-python
!pip install matplotlib
!pip install tensorflow
!pip install scikit-learn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;########################################################
from os import listdir
from os.path import isfile, join
########################################################
import numpy as np
from scipy import signal
from PIL import Image
import cv2
import matplotlib.pyplot as plt
########################################################
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model, layers
from sklearn.model_selection import train_test_split
########################################################
# Comment the following code if you don&#39;t have tensorflow-gpu installed in your enviroment
gpu = len(tf.config.list_physical_devices(&#39;GPU&#39;))&amp;gt;0
print(&amp;quot;GPU is&amp;quot;, &amp;quot;available&amp;quot; if gpu else &amp;quot;NOT AVAILABLE&amp;quot;)
physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;)
tf.config.experimental.set_memory_growth(physical_devices[0], True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GPU is available
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-median-filtering&#34;&gt;3. Median Filtering&lt;/h2&gt;
&lt;p&gt;A simple way to solve this problem is using classic digital image processing techniques since it will not be necessary to train any machine learning algorithms, which usually require more data, time, and better hardware. So, as a first answer, I will apply the median filter to get the background of the image, then I will subtract it from the original image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Denoiser:
    &amp;quot;&amp;quot;&amp;quot;
    Class that creates and object capable of colect dirty images and partially clean some of its noise
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self):
        self.input_image = None
        self.output_image = None
    
    def load_image(self, path):
        # Load an image from the path and return it as a numpy array
        return np.asarray(Image.open(path))/255.0
    
    def save_image(self, dest_path, image):
        #  Take some image array(image) and save it in the destination(dest_path)
        img_arr = np.asarray(image*255.0, dtype=np.uint8)
        Image.fromarray(img_arr).save(dest_path)
        
    def denoiser_pipeline(self, image):
        # Collect background
        background = signal.medfilt2d(image, 11)
        # Select anything that is darker than the background
        foreground_mask = image &amp;lt; background - 0.1
        # Return black for anything that is darker than the background or white otherwise
        output_image = np.where(foreground_mask, 0.0, 1.0)
        
        return output_image
        
    def clean(self, image_path, dest_path):
        # Load the input image
        self.input_image = self.load_image(image_path)
        # Process the image and load it in the output
        self.output_image = self.denoiser_pipeline(self.input_image)
        # Saves the output
        self.save_image(dest_path, self.output_image)
        
    def show(self):
        # Show the last image filtered in a kernel
        in_img = np.asarray(self.input_image*255.0, dtype = np.uint8)
        out_img = np.asarray(self.output_image*255.0, dtype = np.uint8)
        
        fig , axs = plt.subplots(1,2, figsize = (16,16))
        axs[0].imshow(Image.fromarray(in_img), cmap = &#39;gray&#39;)
        axs[1].imshow(Image.fromarray(out_img), cmap = &#39;gray&#39;)
        axs[0].axis(&#39;off&#39;)
        axs[1].axis(&#39;off&#39;)
        plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_datapath = &#39;noisy_data/&#39;
output_datapath = &#39;output_median/&#39;

files = [f for f in listdir(input_datapath) if isfile(join(input_datapath, f))]

denoiser = Denoiser()

for f in files:
    
    denoiser.clean(input_datapath + f, output_datapath + f)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;denoiser.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Analyzing the processed images is possible to see that the algorithm works relatively well, considering its simplicity. It is possible to use it in scenarios without many resources to use more sophisticated and modern solutions.&lt;/p&gt;
&lt;p&gt;The results are available in the output_median directory.&lt;/p&gt;
&lt;h2 id=&#34;4-autoencoder&#34;&gt;4. AutoEncoder&lt;/h2&gt;
&lt;p&gt;A more sophisticated answer to the problem is using the &amp;ldquo;magic&amp;rdquo; of Deep Learning. For this, I will use an Autoencoder network, which is composed of an encoder and a decoder. The encoder compresses the data to a smaller representation. The decoder reconstructs this representation to obtain an output close to the input. During the process, the autoencoder learns the most important features that make up the data.&lt;/p&gt;
&lt;p&gt;Autoencoders can reduce image noise by providing noisy data as an input and outputting the same data without noise. Then, the autoencoder can learn how to filter similar data efficiently.&lt;/p&gt;
&lt;p&gt;But how is it possible to apply this to the proposed problem?&lt;/p&gt;
&lt;p&gt;As stated in the test description, cleaning document images is a well-documented problem. Then it is possible to find databases that provide images with background noise and their respective clean version with ease. After looking for a database that fits the task, I trained an autoencoder using its dirty data as input and its clean images as output. So, I used the network to filter the dirty inputs presented by the challenge.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simple image process to standardize our data
def process_image(path):
    img = cv2.imread(path)
    img = np.asarray(img, dtype=&amp;quot;float32&amp;quot;)
    img = cv2.resize(img, (540, 420))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = img/255.0
    img = np.reshape(img, (420, 540, 1))
    
    return img
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading the training images
noisy_path = &#39;train/&#39;
clean_path = &#39;train_cleaned/&#39;
test_path = &#39;test/&#39;

noisy_dir = listdir(noisy_path)
x_paths = [noisy_path+x for x in noisy_dir]

clean_dir = listdir(clean_path)
y_paths = [clean_path+y for y in clean_dir]

size = (420,540)


X = []
y = []

for image in x_paths:
    
    img = process_image(image)
    X.append(img)

for label in y_paths:
    
    img = process_image(label)
    y.append(img)
    


X = np.array(X)
y = np.array(y)


fig, axs = plt.subplots(1,2, figsize = (16,16))

axs[0].set_title(&#39;Dirty Image&#39;)
axs[0].imshow(X[0][:,:,0], cmap = &#39;gray&#39;)
axs[0].axis(&#39;off&#39;)

axs[1].set_title(&#39;Clean Image&#39;)
axs[1].imshow(y[0][:,:,0], cmap = &#39;gray&#39;)
axs[1].axis(&#39;off&#39;)

plt.show()


print(&amp;quot;Size of X : &amp;quot;, X.shape)
print(&amp;quot;Size of Y : &amp;quot;, y.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Size of X :  (144, 420, 540, 1)
Size of Y :  (144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the construction of the autoencoder, I will use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional layers to extract the significant features of the images;&lt;/li&gt;
&lt;li&gt;Max-pooling for data compression;&lt;/li&gt;
&lt;li&gt;Up-sampling for restoring the data dimension;&lt;/li&gt;
&lt;li&gt;Batch normalization to reduce the difference in the distribution of activations between the layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autoencoder():
    
    
    input_layer = layers.Input(shape=(420,540,1), name= &#39;Image_Input&#39;)
    
    # Encoder
    
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_1&#39;)(input_layer)
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_2&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.MaxPooling2D((2,2), padding = &#39;same&#39;, name=&#39;Pooling_Layer_3&#39;)(x)
    
    
    # Decoder
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_5&#39;)(x)
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_6&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.UpSampling2D((2,2), name = &#39;UpSampling_Layer_3&#39;)(x)
    
    output_layer = layers.Conv2D(1, (3,3), activation = &#39;sigmoid&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_7&#39;)(x)
    
    # Model
    
    autoencoder = Model(inputs = [input_layer], outputs = [output_layer])
    autoencoder.compile(optimizer = &#39;adam&#39;, loss=&#39;mean_squared_error&#39;, metrics=[&#39;mae&#39;])
    
    return autoencoder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ae = autoencoder()
ae.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Image_Input (InputLayer)     [(None, 420, 540, 1)]     0         
_________________________________________________________________
Convolutional_Layer_1 (Conv2 (None, 420, 540, 32)      320       
_________________________________________________________________
Convolutional_Layer_2 (Conv2 (None, 420, 540, 64)      18496     
_________________________________________________________________
batch_normalization (BatchNo (None, 420, 540, 64)      256       
_________________________________________________________________
Pooling_Layer_3 (MaxPooling2 (None, 210, 270, 64)      0         
_________________________________________________________________
Convolutional_Layer_5 (Conv2 (None, 210, 270, 64)      36928     
_________________________________________________________________
Convolutional_Layer_6 (Conv2 (None, 210, 270, 32)      18464     
_________________________________________________________________
batch_normalization_1 (Batch (None, 210, 270, 32)      128       
_________________________________________________________________
UpSampling_Layer_3 (UpSampli (None, 420, 540, 32)      0         
_________________________________________________________________
Convolutional_Layer_7 (Conv2 (None, 420, 540, 1)       289       
=================================================================
Total params: 74,881
Trainable params: 74,689
Non-trainable params: 192
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;keras.utils.plot_model(ae, &amp;quot;assets/autoencoder.png&amp;quot;, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)
print(&amp;quot;Total number of training samples: &amp;quot;, X_train.shape)
print(&amp;quot;Total number of validation samples: &amp;quot;, X_valid.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of training samples:  (129, 420, 540, 1)
Total number of validation samples:  (15, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, patience=20)

hist = ae.fit(X_train, y_train, epochs=50, batch_size=2, validation_data=(X_valid, y_valid), callbacks = [callback])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_loss = hist.history[&#39;loss&#39;]
epoch_val_loss = hist.history[&#39;val_loss&#39;]
epoch_mae = hist.history[&#39;mae&#39;]
epoch_val_mae = hist.history[&#39;val_mae&#39;]

plt.figure(figsize=(20,6))
plt.subplot(1,2,1)
plt.plot(range(0,len(epoch_loss)), epoch_loss, &#39;b-&#39;, linewidth=2, label=&#39;Train Loss&#39;)
plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, &#39;r-&#39;, linewidth=2, label=&#39;Val Loss&#39;)
plt.title(&#39;Evolution of loss on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.subplot(1,2,2)
plt.plot(range(0,len(epoch_mae)), epoch_mae, &#39;b-&#39;, linewidth=2, label=&#39;Train MAE&#39;)
plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, &#39;r-&#39;, linewidth=2,label=&#39;Val MAE&#39;)
plt.title(&#39;Evolution of MAE on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The neural network seems to have learned to solve the problem well for the data sample provided so far, as you can see in the graphs.&lt;/p&gt;
&lt;p&gt;However, this does not mean that the network has learned well for images it has never seen, different noises, rotated texts.&lt;/p&gt;
&lt;p&gt;Then we will test the images on the noisy data provided by the test to check if the algorithm used was efficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_path = &#39;noisy_data/&#39;
test_dir = listdir(test_path)
test_paths = [test_path+x for x in test_dir]

X_test = []
for image in test_paths:
    
    img = process_image(image)
    X_test.append(img)
    
X_test = np.array(X_test)
print(X_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_test = ae.predict(X_test, batch_size=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,25))
for i in range(0,8,2):
    plt.subplot(4,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(X_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Noisy image: {}&#39;.format(test_paths[i]))
    
    plt.subplot(4,2,i+2)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(Y_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Denoised by autoencoder: {}&#39;.format(test_paths[i]))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the images above, the approached strategy was very efficient in removing the noise from the data provided. It seems that the model has learned to distinguish the background from the images well, even with different types of noise and rotated texts.&lt;/p&gt;
&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/denoising-dirty-documents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising Dirty Documents Kaggle Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_14_02_auto_encode.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising with AutoEncoders - T81-558: Applications of Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/michalbrezk/denoise-images-using-autoencoders-tf-keras&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoise images using Autoencoder[TF, Keras] &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SMS Spam Detection with Machine Learning</title>
      <link>https://jpvt.github.io/project/smsspamdetection/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/project/smsspamdetection/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Requirements&lt;/li&gt;
&lt;li&gt;Data Analysis and Feature Engineering&lt;/li&gt;
&lt;li&gt;Data preprocessing&lt;/li&gt;
&lt;li&gt;Classic ML&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-description&#34;&gt;1. Description&lt;/h2&gt;
&lt;p&gt;The SMS Ham-Spam detection dataset is a set of SMS tagged messages that have been collected for SMS Spam research. It contains a set of 5,574 SMS messages in English, considering both train and test data. The tagging standard was defined as &lt;code&gt;ham&lt;/code&gt; (legitimate) or &lt;code&gt;spam&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; files are formatted using the standard of one message per line. Each line is composed by two columns: one with label (&lt;code&gt;ham&lt;/code&gt; or &lt;code&gt;spam&lt;/code&gt;) and other with the raw text. Here are some examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ham   What you doing?how are you?
ham   Ok lar... Joking wif u oni...
ham   dun say so early hor... U c already then say...
ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*
ham   Siva is in hostel aha:-.
ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.
spam   FreeMsg: Txt: CALL to No: 86888 &amp;amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop
spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B
spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Note: messages are not chronologically sorted.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For evaluation purposes, the &lt;code&gt;test&lt;/code&gt; dataset does not prosent the categories (&lt;code&gt;ham&lt;/code&gt;, &lt;code&gt;spam&lt;/code&gt;). Therefore, the &lt;code&gt;train&lt;/code&gt; data is the full source of information for this test.&lt;/p&gt;
&lt;p&gt;The goal of the this test is to achieve a model that can correctly manage the incoming messages on SMS format (&lt;code&gt;ham&lt;/code&gt; or &lt;code&gt;spam&lt;/code&gt;). Considering a real scenario, assume that a regular person does not want to see a &lt;code&gt;spam&lt;/code&gt; message. However, they accepts if a normal message (&lt;code&gt;ham&lt;/code&gt;) is sometimes allocated at the &lt;code&gt;spam&lt;/code&gt; box.&lt;/p&gt;
&lt;h2 id=&#34;2-requirements&#34;&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;!pip install numpy
!pip install matplotlib
!pip install tensorflow
!pip install scikit-learn
!pip install nltk
!pip install transformers
!pip install seaborn
!pip install xgboost
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#################################################################################################

import re
from collections import Counter
import time

#################################################################################################

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#################################################################################################

from sklearn import feature_extraction
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from transformers import TFTrainer, TFTrainingArguments
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from xgboost import XGBClassifier

#################################################################################################

import nltk
from nltk import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#################################################################################################

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model, layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import EarlyStopping

#################################################################################################

gpu = len(tf.config.list_physical_devices(&#39;GPU&#39;))&amp;gt;0

if gpu:
    print(&amp;quot;GPU is&amp;quot;, &amp;quot;available&amp;quot;)
    physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;)
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
else:
    print(&amp;quot;NOT AVAILABLE&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nltk.download(&amp;quot;punkt&amp;quot;)
nltk.download(&amp;quot;stopwords&amp;quot;)
nltk.download(&amp;quot;wordnet&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-data-analysis-and-feature-engineering&#34;&gt;3. Data Analysis and Feature Engineering&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TRAIN_PATH = &#39;TrainingSet/sms-hamspam-train.csv&#39;

df = pd.read_csv(TRAIN_PATH,names = [&#39;class&#39;,&#39;text&#39;], delimiter = &#39;\t&#39;)

df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Go until jurong point, crazy.. Available only ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;SMSSERVICES. for yourinclusive text credits, p...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;25p 4 alfie Moon&#39;s Children in need song on ur...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;U have a secret admirer. REVEAL who thinks U R...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;Dear Voucher Holder, To claim this weeks offer...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4720&lt;/th&gt;
      &lt;td&gt;spam&lt;/td&gt;
      &lt;td&gt;This is the 2nd time we have tried 2 contact u...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4721&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Will √º b going to esplanade fr home?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4722&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Pity, * was in mood for that. So...any other s...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4723&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;The guy did some bitching but I acted like i&#39;d...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4724&lt;/th&gt;
      &lt;td&gt;ham&lt;/td&gt;
      &lt;td&gt;Rofl. Its true to its name&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;4725 rows √ó 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axs = plt.subplots(3, 3, figsize =  (16,16))

# Class Distribution
class_value_counts = df[&#39;class&#39;].value_counts(sort = True)

# Plot
axs[0,0].set_title(&amp;quot;Class Distribution&amp;quot;)
axs[0,0].pie(class_value_counts, labels = class_value_counts.index, autopct = &amp;quot;%1.0f%%&amp;quot;)
axs[0,0].axis(&#39;off&#39;)


# Word Frequency
most_common_ham = pd.DataFrame.from_dict(
    
    Counter(&amp;quot; &amp;quot;.join(df.loc[df[&amp;quot;class&amp;quot;]== &amp;quot;ham&amp;quot;][&amp;quot;text&amp;quot;]).split()).most_common(10)
)

most_common_ham = most_common_ham.rename(columns={0: &amp;quot;word_in_ham&amp;quot;, 1 : &amp;quot;frequency&amp;quot;})


most_common_spam = pd.DataFrame.from_dict(
    
    Counter(&amp;quot; &amp;quot;.join(df.loc[df[&amp;quot;class&amp;quot;]== &amp;quot;spam&amp;quot;][&amp;quot;text&amp;quot;]).split()).most_common(10)
)

most_common_spam = most_common_spam.rename(columns={0: &amp;quot;word_in_spam&amp;quot;, 1 : &amp;quot;frequency&amp;quot;})

axs[0,1].set_title(&amp;quot;Word Frequency in Ham SMS&amp;quot;)
axs[0,1].bar(most_common_ham[&amp;quot;word_in_ham&amp;quot;], most_common_ham[&amp;quot;frequency&amp;quot;])
axs[0,1].set_xticks(np.arange(len(most_common_ham[&amp;quot;word_in_ham&amp;quot;])))
axs[0,1].set_xticklabels(most_common_ham[&amp;quot;word_in_ham&amp;quot;])
axs[0,1].set_xlabel(&amp;quot;Words&amp;quot;)
axs[0,1].set_ylabel(&amp;quot;Frequency&amp;quot;)

axs[0,2].set_title(&amp;quot;Word Frequency in Spam SMS&amp;quot;)
axs[0,2].bar(most_common_spam[&amp;quot;word_in_spam&amp;quot;], most_common_spam[&amp;quot;frequency&amp;quot;], color = &#39;orange&#39;)
axs[0,2].set_xticks(np.arange(len(most_common_spam[&amp;quot;word_in_spam&amp;quot;])))
axs[0,1].set_xticklabels(most_common_spam[&amp;quot;word_in_spam&amp;quot;])
axs[0,2].set_xlabel(&amp;quot;Words&amp;quot;)
axs[0,2].set_ylabel(&amp;quot;Frequency&amp;quot;)

# Length
df[&amp;quot;message_len&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(len)

sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;message_len&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(-50, 250), ax = axs[1,0]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;message_len&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;,ax = axs[1,0])
axs[1,0].set(
    xlabel=&amp;quot;Length&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Length of SMS.&amp;quot;,
)
axs[1,0].legend(loc=&amp;quot;upper right&amp;quot;)


# Number of Words
df[&amp;quot;nwords&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(lambda s: len(re.findall(r&amp;quot;\w+&amp;quot;, s)))

sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;nwords&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(-10, 50), ax = axs[1,1]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;nwords&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;, ax = axs[1,1])

axs[1,1].set(
    xlabel=&amp;quot;Words&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Number of Words in SMS.&amp;quot;,
)
axs[1,1].legend(loc=&amp;quot;upper right&amp;quot;)


# Number of Uppercased Words
df[&amp;quot;nupperwords&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: len(re.findall(r&amp;quot;\b[A-Z][A-Z]+\b&amp;quot;, s))
)
sns.kdeplot(
    df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;nupperwords&amp;quot;],
    shade=True,
    label=&amp;quot;Ham&amp;quot;,
    clip=(0, 35), ax = axs[1,2]
)
sns.kdeplot(df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;nupperwords&amp;quot;], shade=True, label=&amp;quot;Spam&amp;quot;, ax = axs[1,2])
axs[1,2].set(
    xlabel=&amp;quot;Uppercased Words&amp;quot;,
    ylabel=&amp;quot;Density&amp;quot;,
    title=&amp;quot;Number of Uppercased Words.&amp;quot;,
)
axs[1,2].legend(loc=&amp;quot;upper right&amp;quot;)

# Number of Uppercased Characters
df[&amp;quot;nupperchars&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: sum(1 for c in s if c.isupper())
)

sns.scatterplot(x=&amp;quot;message_len&amp;quot;, y=&amp;quot;nupperchars&amp;quot;, hue=&amp;quot;class&amp;quot;, data=df, ax = axs[2,0])
axs[2,0].set(
    xlabel=&amp;quot;Characters&amp;quot;,
    ylabel=&amp;quot;Uppercase Characters&amp;quot;,
    title=&amp;quot;Number of Uppercased Characters in SMS.&amp;quot;,
)
axs[2,0].legend(loc=&amp;quot;upper right&amp;quot;)


# Contains free or win
df[&amp;quot;is_free_or_win&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: int(&amp;quot;free&amp;quot; in s.lower() or &amp;quot;win&amp;quot; in s.lower())
)


grouped_data = (
    df.groupby(&amp;quot;class&amp;quot;)[&amp;quot;is_free_or_win&amp;quot;]
    .value_counts(normalize=True)
    .rename(&amp;quot;Percentage of Group&amp;quot;)
    .reset_index()
)

axs[2,1].set_title(&amp;quot;Distribution of FREE/WIN Words Between Spam and Ham&amp;quot;)

sns.barplot(
    x=&amp;quot;class&amp;quot;,
    y=&amp;quot;Percentage of Group&amp;quot;,
    hue=&amp;quot;is_free_or_win&amp;quot;,
    data=grouped_data, ax = axs[2,1]
)


# Contains url
df[&amp;quot;is_url&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda s: 1
    if re.search(
        r&amp;quot;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&amp;quot;,
        s,
    )
    else 0
)


grouped_data = (
    df.groupby(&amp;quot;class&amp;quot;)[&amp;quot;is_url&amp;quot;]
    .value_counts(normalize=True)
    .rename(&amp;quot;Percentage of Group&amp;quot;)
    .reset_index()
)

axs[2,2].set_title(&amp;quot;Distribution of URL Between Spam and Ham&amp;quot;)

sns.barplot(
    x=&amp;quot;class&amp;quot;,
    y=&amp;quot;Percentage of Group&amp;quot;,
    hue=&amp;quot;is_url&amp;quot;,
    data=grouped_data, ax = axs[2,2]
)

plt.savefig(&#39;assets/data_analysis.jpg&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-data-preprocessing&#34;&gt;4. Data Preprocessing&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: re.sub(r&amp;quot;[^a-zA-Z]+&amp;quot;, &amp;quot; &amp;quot;, row)  
)

df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    Go until jurong point crazy Available only in ...
1    SMSSERVICES for yourinclusive text credits pls...
2     p alfie Moon s Children in need song on ur mo...
3    U have a secret admirer REVEAL who thinks U R ...
4    Dear Voucher Holder To claim this weeks offer ...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(lambda row: word_tokenize(row))
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    [Go, until, jurong, point, crazy, Available, o...
1    [SMSSERVICES, for, yourinclusive, text, credit...
2    [p, alfie, Moon, s, Children, in, need, song, ...
3    [U, have, a, secret, admirer, REVEAL, who, thi...
4    [Dear, Voucher, Holder, To, claim, this, weeks...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: [
        token for token in row if token not in set(stopwords.words(&amp;quot;english&amp;quot;))
    ]
)
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    [Go, jurong, point, crazy, Available, bugis, n...
1    [SMSSERVICES, yourinclusive, text, credits, pl...
2    [p, alfie, Moon, Children, need, song, ur, mob...
3    [U, secret, admirer, REVEAL, thinks, U, R, So,...
4    [Dear, Voucher, Holder, To, claim, weeks, offe...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;text&amp;quot;] = df[&amp;quot;text&amp;quot;].apply(
    lambda row: &amp;quot; &amp;quot;.join([WordNetLemmatizer().lemmatize(word) for word in row])
)
df[&amp;quot;text&amp;quot;].head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    Go jurong point crazy Available bugis n great ...
1    SMSSERVICES yourinclusive text credit pls goto...
2    p alfie Moon Children need song ur mob Tell ur...
3    U secret admirer REVEAL think U R So special C...
4    Dear Voucher Holder To claim week offer PC ple...
Name: text, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[&#39;class&#39;] == &amp;quot;ham&amp;quot;, &amp;quot;class&amp;quot;] = 0
df.loc[df[&#39;class&#39;] == &amp;quot;spam&amp;quot;, &amp;quot;class&amp;quot;] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;message_len&lt;/th&gt;
      &lt;th&gt;nwords&lt;/th&gt;
      &lt;th&gt;nupperwords&lt;/th&gt;
      &lt;th&gt;nupperchars&lt;/th&gt;
      &lt;th&gt;is_free_or_win&lt;/th&gt;
      &lt;th&gt;is_url&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Go jurong point crazy Available bugis n great ...&lt;/td&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;SMSSERVICES yourinclusive text credit pls goto...&lt;/td&gt;
      &lt;td&gt;156&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;p alfie Moon Children need song ur mob Tell ur...&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;U secret admirer REVEAL think U R So special C...&lt;/td&gt;
      &lt;td&gt;147&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Dear Voucher Holder To claim week offer PC ple...&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Do want anytime network min text NEW VIDEO pho...&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;We tried contact offer New Video Phone anytime...&lt;/td&gt;
      &lt;td&gt;155&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Last chance claim ur worth discount voucher Te...&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Urgent call landline Your complimentary Ibiza ...&lt;/td&gt;
      &lt;td&gt;153&lt;/td&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Today Offer Claim ur worth discount voucher Te...&lt;/td&gt;
      &lt;td&gt;158&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;class&#39;] = df[&#39;class&#39;].astype(np.uint8)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
      &lt;th&gt;message_len&lt;/th&gt;
      &lt;th&gt;nwords&lt;/th&gt;
      &lt;th&gt;nupperwords&lt;/th&gt;
      &lt;th&gt;nupperchars&lt;/th&gt;
      &lt;th&gt;is_free_or_win&lt;/th&gt;
      &lt;th&gt;is_url&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
      &lt;td&gt;4725.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;0.132275&lt;/td&gt;
      &lt;td&gt;80.161270&lt;/td&gt;
      &lt;td&gt;16.180106&lt;/td&gt;
      &lt;td&gt;0.646138&lt;/td&gt;
      &lt;td&gt;5.679788&lt;/td&gt;
      &lt;td&gt;0.071111&lt;/td&gt;
      &lt;td&gt;0.003175&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;0.338825&lt;/td&gt;
      &lt;td&gt;60.559271&lt;/td&gt;
      &lt;td&gt;11.947438&lt;/td&gt;
      &lt;td&gt;2.551516&lt;/td&gt;
      &lt;td&gt;11.932286&lt;/td&gt;
      &lt;td&gt;0.257038&lt;/td&gt;
      &lt;td&gt;0.056260&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;36.000000&lt;/td&gt;
      &lt;td&gt;8.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;61.000000&lt;/td&gt;
      &lt;td&gt;12.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;121.000000&lt;/td&gt;
      &lt;td&gt;24.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;910.000000&lt;/td&gt;
      &lt;td&gt;190.000000&lt;/td&gt;
      &lt;td&gt;32.000000&lt;/td&gt;
      &lt;td&gt;129.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 4725 entries, 0 to 4724
Data columns (total 8 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   class           4725 non-null   uint8 
 1   text            4725 non-null   object
 2   message_len     4725 non-null   int64 
 3   nwords          4725 non-null   int64 
 4   nupperwords     4725 non-null   int64 
 5   nupperchars     4725 non-null   int64 
 6   is_free_or_win  4725 non-null   int64 
 7   is_url          4725 non-null   int64 
dtypes: int64(6), object(1), uint8(1)
memory usage: 263.1+ KB
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_val, y_train, y_val = train_test_split(
    df[&amp;quot;text&amp;quot;], df[&amp;quot;class&amp;quot;], test_size=0.1, random_state = 0
)



print(f&amp;quot;Training data: {len(X_train)} (90%)&amp;quot;)
print(f&amp;quot;Testing data: {len(X_val)} (10%)&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Training data: 4252 (90%)
Testing data: 473 (10%)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;5-classic-ml&#34;&gt;5. Classic ML&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

knn = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, KNeighborsClassifier())
        ]
    ),
    {
        &amp;quot;clf__n_neighbors&amp;quot;: (3,5,15,25,45,55),
    }

)

mnbayes = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, MultinomialNB()),
        ]
    ),
    {
        &amp;quot;clf__alpha&amp;quot;: (0.1, 1e-2, 1e-3),
        &amp;quot;clf__fit_prior&amp;quot;: (True, False),
    },
)

svc = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, SVC(gamma=&amp;quot;auto&amp;quot;, C=1000)),
        ]
    ),
    {
        
    }
)


ada = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, AdaBoostClassifier()),
        ]
    ),
    {
        &amp;quot;clf__n_estimators&amp;quot;: [100,200],
        &amp;quot;clf__learning_rate&amp;quot;: [0.001, 0.01, 0.1, 0.2, 0.5]
    }
)

rf = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, RandomForestClassifier()),
        ]
    ),
    {
        &amp;quot;clf__criterion&amp;quot; : [&amp;quot;gini&amp;quot;, &amp;quot;entropy&amp;quot;],
        &amp;quot;clf__max_depth&amp;quot; :   [None, 1,3,5,10],
        &amp;quot;clf__min_samples_split&amp;quot;: [5,10],
        &amp;quot;clf__min_samples_leaf&amp;quot;: [5,10],
        &amp;quot;clf__n_estimators&amp;quot;: [100, 150, 200]
    }
)

xgb = GridSearchCV(
    Pipeline(
        [
            (&amp;quot;BagOfWords&amp;quot;, CountVectorizer()),
            (&amp;quot;Tfidf&amp;quot;, TfidfTransformer()),
            (&amp;quot;clf&amp;quot;, XGBClassifier()),
        ]
    ),
    {
        &#39;clf__max_depth&#39;: [3, 4, 5],
        &amp;quot;clf__n_estimators&amp;quot;: [200, 500, 600]
    }
)


models = {&amp;quot;K-Nearest Neighbors&amp;quot;: knn,
          &amp;quot;Multinomial Naive Bayes&amp;quot;: mnbayes,
          &amp;quot;Support Vector Machine&amp;quot;: svc,
          &amp;quot;AdaBoost&amp;quot;: ada,
          &amp;quot;Random Forest&amp;quot;: rf,
          &amp;quot;XGBoost&amp;quot;: xgb
         }

results = []

for model in models:
    
    print(&amp;quot;\n&amp;quot;,model)
    
    models[model].fit(X= X_train, y = y_train)
    preds = models[model].predict(X_val)
    
    plt.figure(figsize=(10,4))

    heatmap = sns.heatmap(
        data = pd.DataFrame(confusion_matrix(y_val, preds)),
        annot = True,
        fmt = &amp;quot;d&amp;quot;,
        cmap=sns.color_palette(&amp;quot;Blues&amp;quot;, 50),
    )

    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)
    heatmap.yaxis.set_ticklabels(
        heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14
    )

    plt.title(&amp;quot;Confusion Matrix&amp;quot;)
    plt.ylabel(&amp;quot;Ground Truth&amp;quot;)
    plt.xlabel(&amp;quot;Prediction&amp;quot;)
    
    plt.show()
    
    precision = precision_score(y_val, preds)
    recall = recall_score(y_val, preds)
    acc = accuracy_score(y_val, preds)

    print(f&amp;quot;Precision: {precision * 100:.3f}%&amp;quot;)
    print(f&amp;quot;   Recall: {recall * 100 :.3f}%&amp;quot;)
    print(f&amp;quot; Accuracy: {acc * 100:.3f}%&amp;quot;)
    
    results.append([model, precision, recall, acc])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; K-Nearest Neighbors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 100.000%
   Recall: 64.615%
 Accuracy: 95.137%

 Multinomial Naive Bayes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.387%
   Recall: 93.846%
 Accuracy: 98.943%

 Support Vector Machine
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.305%
   Recall: 89.231%
 Accuracy: 98.309%

 AdaBoost
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_7.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.361%
   Recall: 92.308%
 Accuracy: 98.732%

 Random Forest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_9.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 98.148%
   Recall: 81.538%
 Accuracy: 97.252%

 XGBoost
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_25_11.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 96.610%
   Recall: 87.692%
 Accuracy: 97.886%
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;6-lstm&#34;&gt;6. LSTM&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_words = 1000
max_len = 150
tok = Tokenizer(num_words=max_words)
tok.fit_on_texts(X_train)
sequences = tok.texts_to_sequences(X_train)
sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sequences_val = tok.texts_to_sequences(X_val)
sequences_matrix_val = sequence.pad_sequences(sequences_val, maxlen = max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_rnn():
    
    input_layer = layers.Input(shape = [max_len], name=&#39;Input_Layer&#39;)
    
    x = layers.Embedding(max_words, 50, input_length = max_len, name = &amp;quot;Embedding_Layer&amp;quot;)(input_layer)
    x = layers.LSTM(64, activation = &#39;tanh&#39;, name = &amp;quot;LSTM_Layer&amp;quot;)(x)
    x = layers.Dense(256, activation = &#39;relu&#39;, name = &#39;Dense_Layer_1&#39;)(x)
    #x = layers.Dropout(0.5)(x)
    output_layer = layers.Dense(1, activation=&#39;sigmoid&#39;,name = &#39;Output_Layer&#39;)(x)
    
    rnn = Model(inputs=input_layer, outputs=output_layer)
    
    return rnn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = build_rnn()
model.summary()
keras.utils.plot_model(model, &amp;quot;assets/simple_rnn.png&amp;quot;, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input_Layer (InputLayer)     [(None, 150)]             0         
_________________________________________________________________
Embedding_Layer (Embedding)  (None, 150, 50)           50000     
_________________________________________________________________
LSTM_Layer (LSTM)            (None, 64)                29440     
_________________________________________________________________
Dense_Layer_1 (Dense)        (None, 256)               16640     
_________________________________________________________________
Output_Layer (Dense)         (None, 1)                 257       
=================================================================
Total params: 96,337
Trainable params: 96,337
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_30_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(loss = &#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;,
              metrics =[&#39;accuracy&#39;,keras.metrics.Precision(),keras.metrics.Recall()])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;early_stopping = keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0.0001)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hist = model.fit(sequences_matrix, y_train, batch_size = 128, epochs = 10,
                 validation_data=(sequences_matrix_val, y_val))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_loss = hist.history[&#39;loss&#39;]
epoch_val_loss = hist.history[&#39;val_loss&#39;]

epoch_acc = hist.history[&#39;accuracy&#39;]
epoch_val_acc = hist.history[&#39;val_accuracy&#39;]

epoch_precision = hist.history[&#39;precision&#39;]
epoch_val_precision = hist.history[&#39;val_precision&#39;]

epoch_recall = hist.history[&#39;recall&#39;]
epoch_val_recall = hist.history[&#39;val_recall&#39;]


fig, axs = plt.subplots(2,2, figsize = (16,16))

plt.figure(figsize=(20,6))

axs[0,0].plot(range(0,len(epoch_loss)), epoch_loss, &#39;b-&#39;, linewidth=2, label=&#39;Train Loss&#39;)
axs[0,0].plot(range(0,len(epoch_val_loss)), epoch_val_loss, &#39;r-&#39;, linewidth=2, label=&#39;Val Loss&#39;)
axs[0,0].set_title(&#39;Evolution of loss on train &amp;amp; validation datasets over epochs&#39;)
axs[0,0].legend(loc=&#39;best&#39;)


axs[0,1].plot(range(0,len(epoch_acc)), epoch_acc, &#39;b-&#39;, linewidth=2, label=&#39;Train accuracy&#39;)
axs[0,1].plot(range(0,len(epoch_val_acc)), epoch_val_acc, &#39;r-&#39;, linewidth=2,label=&#39;Val accuracy&#39;)
axs[0,1].set_title(&#39;Evolution of accuracy on train &amp;amp; validation datasets over epochs&#39;)
axs[0,1].legend(loc=&#39;best&#39;)


axs[1,0].plot(range(0,len(epoch_precision)), epoch_precision, &#39;b-&#39;, linewidth=2, label=&#39;Train precision&#39;)
axs[1,0].plot(range(0,len(epoch_val_precision)), epoch_val_precision, &#39;r-&#39;, linewidth=2, label=&#39;Val precision&#39;)
axs[1,0].set_title(&#39;Evolution of precision on train &amp;amp; validation datasets over epochs&#39;)
axs[1,0].legend(loc=&#39;best&#39;)


axs[1,1].plot(range(0,len(epoch_recall)), epoch_recall, &#39;b-&#39;, linewidth=2, label=&#39;Train recall&#39;)
axs[1,1].plot(range(0,len(epoch_val_recall)), epoch_val_recall, &#39;r-&#39;, linewidth=2,label=&#39;Val recall&#39;)
axs[1,1].set_title(&#39;Evolution of recall on train &amp;amp; validation datasets over epochs&#39;)
axs[1,1].legend(loc=&#39;best&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1440x432 with 0 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;preds = model.predict(sequences_matrix_val)
preds = np.uint8(np.round(preds.T))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(10,4))

heatmap = sns.heatmap(
    data = pd.DataFrame(confusion_matrix(y_val.values, preds[0])),
    annot = True,
    fmt = &amp;quot;d&amp;quot;,
    cmap=sns.color_palette(&amp;quot;Blues&amp;quot;, 50),
)

heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)
heatmap.yaxis.set_ticklabels(
    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14
)

plt.title(&amp;quot;Confusion Matrix&amp;quot;)
plt.ylabel(&amp;quot;Ground Truth&amp;quot;)
plt.xlabel(&amp;quot;Prediction&amp;quot;)

plt.show()

precision = precision_score(y_val.values, preds[0])
recall = recall_score(y_val.values, preds[0])
acc = accuracy_score(y_val.values, preds[0])

print(f&amp;quot;Precision: {precision * 100:.3f}%&amp;quot;)
print(f&amp;quot;   Recall: {recall * 100 :.3f}%&amp;quot;)
print(f&amp;quot; Accuracy: {acc * 100:.3f}%&amp;quot;)

results.append([&#39;Simple RNN&#39;, precision, recall, acc])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision: 96.875%
   Recall: 95.385%
 Accuracy: 98.943%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.save(f&#39;saved_models/my_model_{time.time()}.h5&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results = pd.DataFrame(results, columns=[&#39;Algorithm&#39;, &#39;Precision&#39;, &#39;Recall&#39;, &#39;Accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Algorithm&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
      &lt;th&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;K-Nearest Neighbors&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.646154&lt;/td&gt;
      &lt;td&gt;0.951374&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Multinomial Naive Bayes&lt;/td&gt;
      &lt;td&gt;0.983871&lt;/td&gt;
      &lt;td&gt;0.938462&lt;/td&gt;
      &lt;td&gt;0.989429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Support Vector Machine&lt;/td&gt;
      &lt;td&gt;0.983051&lt;/td&gt;
      &lt;td&gt;0.892308&lt;/td&gt;
      &lt;td&gt;0.983087&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;AdaBoost&lt;/td&gt;
      &lt;td&gt;0.983607&lt;/td&gt;
      &lt;td&gt;0.923077&lt;/td&gt;
      &lt;td&gt;0.987315&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;0.981481&lt;/td&gt;
      &lt;td&gt;0.815385&lt;/td&gt;
      &lt;td&gt;0.972516&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;XGBoost&lt;/td&gt;
      &lt;td&gt;0.966102&lt;/td&gt;
      &lt;td&gt;0.876923&lt;/td&gt;
      &lt;td&gt;0.978858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Simple RNN&lt;/td&gt;
      &lt;td&gt;0.968750&lt;/td&gt;
      &lt;td&gt;0.953846&lt;/td&gt;
      &lt;td&gt;0.989429&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;evaluating-the-test&#34;&gt;Evaluating the test&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TEST_PATH = &#39;TestSet/sms-hamspam-test.csv&#39;

df_test = pd.read_csv(TEST_PATH,names = [&#39;text&#39;])
df_test
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;I know that my friend already told that.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;It took Mr owl 3 licks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Dunno y u ask me.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;K.k:)advance happy pongal.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;I know but you need to get hotel now. I just g...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;842&lt;/th&gt;
      &lt;td&gt;Booked ticket for pongal?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;843&lt;/th&gt;
      &lt;td&gt;Yes :)it completely in out of form:)clark also...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;844&lt;/th&gt;
      &lt;td&gt;Yeah sure&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;845&lt;/th&gt;
      &lt;td&gt;He is there. You call and meet him&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;846&lt;/th&gt;
      &lt;td&gt;I see a cup of coffee animation&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;847 rows √ó 1 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def apply_data_preprocessing(dataframe, column):
    
    df = dataframe.copy()
    
    df[column] = df[column].apply(
        lambda row: re.sub(r&amp;quot;[^a-zA-Z]+&amp;quot;, &amp;quot; &amp;quot;, row)  
    )
    
    df[column] = df[column].apply(lambda row: word_tokenize(row))
    
    df[column] = df[column].apply(
        lambda row: [
            token for token in row if token not in set(stopwords.words(&amp;quot;english&amp;quot;))
        ]
    )
    
    df[column] = df[column].apply(
        lambda row: &amp;quot; &amp;quot;.join([WordNetLemmatizer().lemmatize(word) for word in row])
    )
    
    X = df[column].values
    
    max_words = 1000
    max_len = 150
    tok = Tokenizer(num_words=max_words)
    tok.fit_on_texts(X)
    sequences = tok.texts_to_sequences(X)
    sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)
    
    return sequences_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def classify(text_csv, column, model_path):
    
    df_test = pd.read_csv(text_csv,names = [column])
    
    sequences_matrix = apply_data_preprocessing(df_test, column)
    
    model = tf.keras.models.load_model(model_path)
    
    preds = model.predict(sequences_matrix)
    preds = np.uint8(np.round(preds.T))
    preds = preds[0]
    
    df_test[&#39;class&#39;] = preds
    
    return df_test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;7-references&#34;&gt;7. References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/uciml/sms-spam-collection-dataset/notebooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMS Spam Detection - Kaggle notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
