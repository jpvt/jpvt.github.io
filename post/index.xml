<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Jo√£o Pedro Vasconcelos</title>
    <link>https://jpvt.github.io/post/</link>
      <atom:link href="https://jpvt.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jpvt.github.io/media/icon_hu6e0ba196f4e113be19f679f5e0d6caf1_39320_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://jpvt.github.io/post/</link>
    </image>
    
    <item>
      <title>Denoising Documents with Computer Vision and Digital Image Processing</title>
      <link>https://jpvt.github.io/post/documentcleanup/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/post/documentcleanup/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Requirements&lt;/li&gt;
&lt;li&gt;Median Filtering&lt;/li&gt;
&lt;li&gt;AutoEncoder&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-description&#34;&gt;1. Description&lt;/h2&gt;
&lt;p&gt;Many image processing applications make use of digitalized textual data. However, the presence of any type of noise can create difficulties in post-processing information, such as on OCR detection. To improve the information manipulation on such data, a previous image processing step is required.&lt;/p&gt;
&lt;p&gt;In light of this idea, a set of text paragraphs containing plain English language was collected. Different font styles, size, and background noise level were arranged to simulate the a variety of scenarios.&lt;/p&gt;
&lt;p&gt;The objective of this article is to evaluate the possible image processing methods that could fix the text samples. Note that the samples have a different type of background noise and present a set of text fonts. Therefore, the candidate should provide a flexible algorithm that can correctly detect what is text characters and background noise, offering a clean version of each text paragraph as result.&lt;/p&gt;
&lt;h2 id=&#34;2-requirements&#34;&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;Assuming that you have a clean enviroment to run this jupyter notebook, create a new code block, copy and paste the following code and run it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;!pip install numpy
!pip install scipy
!pip install pillow
!pip install opencv-python
!pip install matplotlib
!pip install tensorflow
!pip install scikit-learn
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;########################################################
from os import listdir
from os.path import isfile, join
########################################################
import numpy as np
from scipy import signal
from PIL import Image
import cv2
import matplotlib.pyplot as plt
########################################################
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential, Model, layers
from sklearn.model_selection import train_test_split
########################################################
# Comment the following code if you don&#39;t have tensorflow-gpu installed in your enviroment
gpu = len(tf.config.list_physical_devices(&#39;GPU&#39;))&amp;gt;0
print(&amp;quot;GPU is&amp;quot;, &amp;quot;available&amp;quot; if gpu else &amp;quot;NOT AVAILABLE&amp;quot;)
physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;)
tf.config.experimental.set_memory_growth(physical_devices[0], True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GPU is available
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-median-filtering&#34;&gt;3. Median Filtering&lt;/h2&gt;
&lt;p&gt;A simple way to solve this problem is using classic digital image processing techniques since it will not be necessary to train any machine learning algorithms, which usually require more data, time, and better hardware. So, as a first answer, I will apply the median filter to get the background of the image, then I will subtract it from the original image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Denoiser:
    &amp;quot;&amp;quot;&amp;quot;
    Class that creates and object capable of colect dirty images and partially clean some of its noise
    &amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self):
        self.input_image = None
        self.output_image = None
    
    def load_image(self, path):
        # Load an image from the path and return it as a numpy array
        return np.asarray(Image.open(path))/255.0
    
    def save_image(self, dest_path, image):
        #  Take some image array(image) and save it in the destination(dest_path)
        img_arr = np.asarray(image*255.0, dtype=np.uint8)
        Image.fromarray(img_arr).save(dest_path)
        
    def denoiser_pipeline(self, image):
        # Collect background
        background = signal.medfilt2d(image, 11)
        # Select anything that is darker than the background
        foreground_mask = image &amp;lt; background - 0.1
        # Return black for anything that is darker than the background or white otherwise
        output_image = np.where(foreground_mask, 0.0, 1.0)
        
        return output_image
        
    def clean(self, image_path, dest_path):
        # Load the input image
        self.input_image = self.load_image(image_path)
        # Process the image and load it in the output
        self.output_image = self.denoiser_pipeline(self.input_image)
        # Saves the output
        self.save_image(dest_path, self.output_image)
        
    def show(self):
        # Show the last image filtered in a kernel
        in_img = np.asarray(self.input_image*255.0, dtype = np.uint8)
        out_img = np.asarray(self.output_image*255.0, dtype = np.uint8)
        
        fig , axs = plt.subplots(1,2, figsize = (16,16))
        axs[0].imshow(Image.fromarray(in_img), cmap = &#39;gray&#39;)
        axs[1].imshow(Image.fromarray(out_img), cmap = &#39;gray&#39;)
        axs[0].axis(&#39;off&#39;)
        axs[1].axis(&#39;off&#39;)
        plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_datapath = &#39;noisy_data/&#39;
output_datapath = &#39;output_median/&#39;

files = [f for f in listdir(input_datapath) if isfile(join(input_datapath, f))]

denoiser = Denoiser()

for f in files:
    
    denoiser.clean(input_datapath + f, output_datapath + f)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;denoiser.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Analyzing the processed images is possible to see that the algorithm works relatively well, considering its simplicity. It is possible to use it in scenarios without many resources to use more sophisticated and modern solutions.&lt;/p&gt;
&lt;p&gt;The results are available in the output_median directory.&lt;/p&gt;
&lt;h2 id=&#34;4-autoencoder&#34;&gt;4. AutoEncoder&lt;/h2&gt;
&lt;p&gt;A more sophisticated answer to the problem is using the &amp;ldquo;magic&amp;rdquo; of Deep Learning. For this, I will use an Autoencoder network, which is composed of an encoder and a decoder. The encoder compresses the data to a smaller representation. The decoder reconstructs this representation to obtain an output close to the input. During the process, the autoencoder learns the most important features that make up the data.&lt;/p&gt;
&lt;p&gt;Autoencoders can reduce image noise by providing noisy data as an input and outputting the same data without noise. Then, the autoencoder can learn how to filter similar data efficiently.&lt;/p&gt;
&lt;p&gt;But how is it possible to apply this to the proposed problem?&lt;/p&gt;
&lt;p&gt;As stated in the test description, cleaning document images is a well-documented problem. Then it is possible to find databases that provide images with background noise and their respective clean version with ease. After looking for a database that fits the task, I trained an autoencoder using its dirty data as input and its clean images as output. So, I used the network to filter the dirty inputs presented by the challenge.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simple image process to standardize our data
def process_image(path):
    img = cv2.imread(path)
    img = np.asarray(img, dtype=&amp;quot;float32&amp;quot;)
    img = cv2.resize(img, (540, 420))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = img/255.0
    img = np.reshape(img, (420, 540, 1))
    
    return img
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading the training images
noisy_path = &#39;train/&#39;
clean_path = &#39;train_cleaned/&#39;
test_path = &#39;test/&#39;

noisy_dir = listdir(noisy_path)
x_paths = [noisy_path+x for x in noisy_dir]

clean_dir = listdir(clean_path)
y_paths = [clean_path+y for y in clean_dir]

size = (420,540)


X = []
y = []

for image in x_paths:
    
    img = process_image(image)
    X.append(img)

for label in y_paths:
    
    img = process_image(label)
    y.append(img)
    


X = np.array(X)
y = np.array(y)


fig, axs = plt.subplots(1,2, figsize = (16,16))

axs[0].set_title(&#39;Dirty Image&#39;)
axs[0].imshow(X[0][:,:,0], cmap = &#39;gray&#39;)
axs[0].axis(&#39;off&#39;)

axs[1].set_title(&#39;Clean Image&#39;)
axs[1].imshow(y[0][:,:,0], cmap = &#39;gray&#39;)
axs[1].axis(&#39;off&#39;)

plt.show()


print(&amp;quot;Size of X : &amp;quot;, X.shape)
print(&amp;quot;Size of Y : &amp;quot;, y.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Size of X :  (144, 420, 540, 1)
Size of Y :  (144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the construction of the autoencoder, I will use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional layers to extract the significant features of the images;&lt;/li&gt;
&lt;li&gt;Max-pooling for data compression;&lt;/li&gt;
&lt;li&gt;Up-sampling for restoring the data dimension;&lt;/li&gt;
&lt;li&gt;Batch normalization to reduce the difference in the distribution of activations between the layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def autoencoder():
    
    
    input_layer = layers.Input(shape=(420,540,1), name= &#39;Image_Input&#39;)
    
    # Encoder
    
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_1&#39;)(input_layer)
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_2&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.MaxPooling2D((2,2), padding = &#39;same&#39;, name=&#39;Pooling_Layer_3&#39;)(x)
    
    
    # Decoder
    x = layers.Conv2D(64, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_5&#39;)(x)
    x = layers.Conv2D(32, (3,3), activation = &#39;relu&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_6&#39;)(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.UpSampling2D((2,2), name = &#39;UpSampling_Layer_3&#39;)(x)
    
    output_layer = layers.Conv2D(1, (3,3), activation = &#39;sigmoid&#39;, padding = &#39;same&#39;, name = &#39;Convolutional_Layer_7&#39;)(x)
    
    # Model
    
    autoencoder = Model(inputs = [input_layer], outputs = [output_layer])
    autoencoder.compile(optimizer = &#39;adam&#39;, loss=&#39;mean_squared_error&#39;, metrics=[&#39;mae&#39;])
    
    return autoencoder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ae = autoencoder()
ae.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Image_Input (InputLayer)     [(None, 420, 540, 1)]     0         
_________________________________________________________________
Convolutional_Layer_1 (Conv2 (None, 420, 540, 32)      320       
_________________________________________________________________
Convolutional_Layer_2 (Conv2 (None, 420, 540, 64)      18496     
_________________________________________________________________
batch_normalization (BatchNo (None, 420, 540, 64)      256       
_________________________________________________________________
Pooling_Layer_3 (MaxPooling2 (None, 210, 270, 64)      0         
_________________________________________________________________
Convolutional_Layer_5 (Conv2 (None, 210, 270, 64)      36928     
_________________________________________________________________
Convolutional_Layer_6 (Conv2 (None, 210, 270, 32)      18464     
_________________________________________________________________
batch_normalization_1 (Batch (None, 210, 270, 32)      128       
_________________________________________________________________
UpSampling_Layer_3 (UpSampli (None, 420, 540, 32)      0         
_________________________________________________________________
Convolutional_Layer_7 (Conv2 (None, 420, 540, 1)       289       
=================================================================
Total params: 74,881
Trainable params: 74,689
Non-trainable params: 192
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;keras.utils.plot_model(ae, &amp;quot;assets/autoencoder.png&amp;quot;, show_shapes=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)
print(&amp;quot;Total number of training samples: &amp;quot;, X_train.shape)
print(&amp;quot;Total number of validation samples: &amp;quot;, X_valid.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Total number of training samples:  (129, 420, 540, 1)
Total number of validation samples:  (15, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;callback = keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, patience=20)

hist = ae.fit(X_train, y_train, epochs=50, batch_size=2, validation_data=(X_valid, y_valid), callbacks = [callback])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_loss = hist.history[&#39;loss&#39;]
epoch_val_loss = hist.history[&#39;val_loss&#39;]
epoch_mae = hist.history[&#39;mae&#39;]
epoch_val_mae = hist.history[&#39;val_mae&#39;]

plt.figure(figsize=(20,6))
plt.subplot(1,2,1)
plt.plot(range(0,len(epoch_loss)), epoch_loss, &#39;b-&#39;, linewidth=2, label=&#39;Train Loss&#39;)
plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, &#39;r-&#39;, linewidth=2, label=&#39;Val Loss&#39;)
plt.title(&#39;Evolution of loss on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.subplot(1,2,2)
plt.plot(range(0,len(epoch_mae)), epoch_mae, &#39;b-&#39;, linewidth=2, label=&#39;Train MAE&#39;)
plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, &#39;r-&#39;, linewidth=2,label=&#39;Val MAE&#39;)
plt.title(&#39;Evolution of MAE on train &amp;amp; validation datasets over epochs&#39;)
plt.legend(loc=&#39;best&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_23_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The neural network seems to have learned to solve the problem well for the data sample provided so far, as you can see in the graphs.&lt;/p&gt;
&lt;p&gt;However, this does not mean that the network has learned well for images it has never seen, different noises, rotated texts.&lt;/p&gt;
&lt;p&gt;Then we will test the images on the noisy data provided by the test to check if the algorithm used was efficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_path = &#39;noisy_data/&#39;
test_dir = listdir(test_path)
test_paths = [test_path+x for x in test_dir]

X_test = []
for image in test_paths:
    
    img = process_image(image)
    X_test.append(img)
    
X_test = np.array(X_test)
print(X_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(144, 420, 540, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Y_test = ae.predict(X_test, batch_size=4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(15,25))
for i in range(0,8,2):
    plt.subplot(4,2,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(X_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Noisy image: {}&#39;.format(test_paths[i]))
    
    plt.subplot(4,2,i+2)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(Y_test[i][:,:,0], cmap=&#39;gray&#39;)
    plt.title(&#39;Denoised by autoencoder: {}&#39;.format(test_paths[i]))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the images above, the approached strategy was very efficient in removing the noise from the data provided. It seems that the model has learned to distinguish the background from the images well, even with different types of noise and rotated texts.&lt;/p&gt;
&lt;h2 id=&#34;5-references&#34;&gt;5. References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/denoising-dirty-documents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising Dirty Documents Kaggle Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_14_02_auto_encode.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoising with AutoEncoders - T81-558: Applications of Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/michalbrezk/denoise-images-using-autoencoders-tf-keras&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denoise images using Autoencoder[TF, Keras] &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Metro Interstate Traffic Volume Analysis</title>
      <link>https://jpvt.github.io/post/metro_traffic_volume/</link>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://jpvt.github.io/post/metro_traffic_volume/</guid>
      <description>&lt;h1 id=&#34;metro-interstate-traffic-volume-analysis&#34;&gt;Metro Interstate Traffic Volume Analysis&lt;/h1&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this notebook, I will discuss the results of my second assignment of the class Introduction to Artificial Intelligence. My job was to predict the traffic volume on Metro Interstate with machine learning models. Finally, I will discuss my results and present some insights into the data.&lt;/p&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset used for this assignment is used by many people all over the world, mainly for learning purposes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About:&lt;/strong&gt;
Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.&lt;/p&gt;
&lt;p&gt;This dataset is available at: &lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00492/&#34;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/00492/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;importing-packages&#34;&gt;Importing Packages&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data Preprocessing Packages
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import re

# Data Visualization Packages
import matplotlib.pyplot as plt
import seaborn as sns
import pandas_profiling as pf
from sklearn.ensemble import RandomForestRegressor

# Models and Metrics
from sklearn import model_selection, svm
from sklearn.svm import SVR
from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score, cross_val_predict,GridSearchCV, StratifiedKFold, KFold, RandomizedSearchCV, train_test_split

import xgboost as xgb
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;getting-the-data&#34;&gt;Getting the data:&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = False

if dataset:
    df = pd.read_csv(&amp;quot;metro.csv&amp;quot;)
else:
    df = pd.read_csv(&amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz&amp;quot;)


df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;rain_1h&lt;/th&gt;
      &lt;th&gt;snow_1h&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;weather_description&lt;/th&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;scattered clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 09:00:00&lt;/td&gt;
      &lt;td&gt;5545&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 10:00:00&lt;/td&gt;
      &lt;td&gt;4516&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 11:00:00&lt;/td&gt;
      &lt;td&gt;4767&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 12:00:00&lt;/td&gt;
      &lt;td&gt;5026&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 13:00:00&lt;/td&gt;
      &lt;td&gt;4918&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info() #Basic information about each column
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 48204 entries, 0 to 48203
Data columns (total 9 columns):
holiday                48204 non-null object
temp                   48204 non-null float64
rain_1h                48204 non-null float64
snow_1h                48204 non-null float64
clouds_all             48204 non-null int64
weather_main           48204 non-null object
weather_description    48204 non-null object
date_time              48204 non-null object
traffic_volume         48204 non-null int64
dtypes: float64(3), int64(2), object(4)
memory usage: 3.3+ MB
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;descri√ß√µes-do-dataset-valores-num√©ricos-e-categ√≥ricos&#34;&gt;Descri√ß√µes do dataset, valores num√©ricos e categ√≥ricos&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;rain_1h&lt;/th&gt;
      &lt;th&gt;snow_1h&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;48204.000000&lt;/td&gt;
      &lt;td&gt;48204.000000&lt;/td&gt;
      &lt;td&gt;48204.000000&lt;/td&gt;
      &lt;td&gt;48204.000000&lt;/td&gt;
      &lt;td&gt;48204.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;281.205870&lt;/td&gt;
      &lt;td&gt;0.334264&lt;/td&gt;
      &lt;td&gt;0.000222&lt;/td&gt;
      &lt;td&gt;49.362231&lt;/td&gt;
      &lt;td&gt;3259.818355&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;13.338232&lt;/td&gt;
      &lt;td&gt;44.789133&lt;/td&gt;
      &lt;td&gt;0.008168&lt;/td&gt;
      &lt;td&gt;39.015750&lt;/td&gt;
      &lt;td&gt;1986.860670&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;272.160000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1193.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;282.450000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;64.000000&lt;/td&gt;
      &lt;td&gt;3380.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;291.806000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;90.000000&lt;/td&gt;
      &lt;td&gt;4933.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;310.070000&lt;/td&gt;
      &lt;td&gt;9831.300000&lt;/td&gt;
      &lt;td&gt;0.510000&lt;/td&gt;
      &lt;td&gt;100.000000&lt;/td&gt;
      &lt;td&gt;7280.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.describe(include= &#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;weather_description&lt;/th&gt;
      &lt;th&gt;date_time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;48204&lt;/td&gt;
      &lt;td&gt;48204&lt;/td&gt;
      &lt;td&gt;48204&lt;/td&gt;
      &lt;td&gt;48204&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;unique&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;40575&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;top&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;sky is clear&lt;/td&gt;
      &lt;td&gt;2013-04-18 22:00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;freq&lt;/th&gt;
      &lt;td&gt;48143&lt;/td&gt;
      &lt;td&gt;15164&lt;/td&gt;
      &lt;td&gt;11665&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis:&lt;/h2&gt;
&lt;p&gt;It is extremely important to know the dataset well since in machine learning is the diversity in the experience that will guarantee success in carrying out a given task. Statistics in this process is very useful, as it provides descriptive measures that demonstrate the main characteristics of the data we are dealing with. Additionally, searching for information about the data and the problem to which it is linked can be of great help and even essential to improve the achievement of the desired task.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;div&gt;
&lt;img src=&#34;bob_eda.jpg&#34; width=&#34;400&#34;/&gt;
&lt;/div&gt;
&lt;h3 id=&#34;about-the-data&#34;&gt;About the data:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Holiday: Indicates if the date is a holiday and if it specifies the holiday, if not &lt;strong&gt;None&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Temp: Indicates the temperature in Kelvin.&lt;/li&gt;
&lt;li&gt;rain_1h: Amount in mm of rain that occurred in the hour.&lt;/li&gt;
&lt;li&gt;snow_1h: Amount in mm of snow that occurred in the hour.&lt;/li&gt;
&lt;li&gt;clouds_all: Percentage of cloud cover.&lt;/li&gt;
&lt;li&gt;weather_main: Short textual description of the current weather.&lt;/li&gt;
&lt;li&gt;weather_description:  Longer textual description of the current weather.&lt;/li&gt;
&lt;li&gt;date_time: Hour of the data collected in local CST time.&lt;/li&gt;
&lt;li&gt;traffic_volume: Hourly I-94 ATR 301 reported westbound traffic volume.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;dispon√≠vel em: &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first observations to be made with the information we have so far are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is no missing data, but that does not mean that there is no inconsistent data.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The date_time, a time stamp, is not defined as the pandas&#39; timestamp. The way it was extracted will not bring us any information and that is a big problem. Since in most cities, traffic occurs at rush hour.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;There are temperature records at absolute zero, clearly inconsistent data.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;rain_1h and snow_1h have many zeros and their distribution is not very well defined, since in rare moments they have high records.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;99.9% of Holiday data is None, and the other data is spread over multiple holidays&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;holiday&#34;&gt;Holiday&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = (16,6))
sns.countplot(y = df[&#39;holiday&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_13_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As normal days are much more frequent over the years of the dataset, then holidays are not visible in the Plot. To analyze them it is necessary to remove the normal days. Then I will categorize between Holiday and Non-Holiday&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;h_df = df[df[&#39;holiday&#39;] != &#39;None&#39;]
plt.figure(figsize = (16,6))
sns.countplot(y = h_df[&#39;holiday&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;To solve this problem, I will recategorize all holidays as Holiday and leave them in the same category.&lt;/p&gt;
&lt;h3 id=&#34;temperature&#34;&gt;Temperature&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = (16,8))
sns.boxplot(df[&#39;temp&#39;])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_18_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As seen before, some Outliers are at absolute zero, probably due to some error in capturing the temperature. Then I will remove the Outliers so as not to affect my result.&lt;/p&gt;
&lt;h3 id=&#34;rain-and-snow&#34;&gt;Rain and Snow&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = (16,6))
sns.distplot(df[&#39;rain_1h&#39;], kde = True)
plt.show()

plt.figure(figsize = (16,6))
sns.distplot(df[&#39;snow_1h&#39;], kde = True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output_21_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the features have many zeros and the distribution is skewed.&lt;/p&gt;
&lt;h3 id=&#34;clouds&#34;&gt;Clouds&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = (16,6))
sns.distplot(df[&#39;clouds_all&#39;],)
plt.show()
df[&#39;clouds_all&#39;].describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;count    48204.000000
mean        49.362231
std         39.015750
min          0.000000
25%          1.000000
50%         64.000000
75%         90.000000
max        100.000000
Name: clouds_all, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It has many zeros, but does not appear to show any inconsistent data.&lt;/p&gt;
&lt;h3 id=&#34;weather-main-e-weather-description&#34;&gt;Weather Main e Weather Description&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize = (16,6))
sns.countplot(y = df[&#39;weather_main&#39;])
plt.show()

plt.figure(figsize = (16,8))
sns.countplot(y =df[&#39;weather_description&#39;])
plt.show()

confusion_matrix = pd.crosstab(df[&#39;weather_main&#39;], df[&#39;weather_description&#39;])
confusion_matrix.corr(method = &#39;spearman&#39;).style.background_gradient(cmap=&#39;coolwarm&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;output_27_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since weather_description offers only an extension of the description of weather_main, it is probably not of interest to keep both for modeling. Since they carry correlated information, which can be seen in the matrix above.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that the exploratory data analysis has been done, it will be necessary to clean the dataset, to guarantee the success of the model. The observations made at the EDA will be made below.&lt;/p&gt;
&lt;h3 id=&#34;holiday-parsing&#34;&gt;Holiday Parsing&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def holiday(holiday):
    cat = &#39;None&#39;
    if holiday != &#39;None&#39;:
        cat = &#39;Holiday&#39;
        
    return cat

df[&#39;holiday&#39;] = df[&#39;holiday&#39;].map(holiday)
df[&#39;holiday&#39;].unique()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;None&#39;, &#39;Holiday&#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;parsing-do-date_time&#34;&gt;Parsing do date_time&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def parse_timestamp(df, datetime):
    df[datetime] = pd.to_datetime(df[datetime])
    df[&#39;Year&#39;] = df[datetime].dt.year
    df[&#39;Month&#39;] = df[datetime].dt.month
    df[&#39;Weekday&#39;] = df[datetime].dt.weekday
    df[&#39;Hour&#39;] = df[datetime].dt.hour
    
def categorize_hour(hour):    
    cat = &#39;None&#39;
    
    if hour in [1,2,3,4,5]:
        cat = &#39;dawn&#39;
    elif hour in [6,7,8,9,10,11,12]:
        cat = &#39;morning&#39;
    elif hour in [13,14,15,16,17,18]:
        cat = &#39;afternoon&#39;
    elif hour in [19,20,21,22,23,24]:
        cat = &#39;night&#39;
        
    return cat   
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before parsing the date_time i will remove possible duplicates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.drop_duplicates(inplace = True)
df.duplicated().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;parse_timestamp(df, &#39;date_time&#39;)
#df[&#39;Hour&#39;] = df[&#39;Hour&#39;].map(categorize_hour)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;rain_1h&lt;/th&gt;
      &lt;th&gt;snow_1h&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;weather_description&lt;/th&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;scattered clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 09:00:00&lt;/td&gt;
      &lt;td&gt;5545&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 10:00:00&lt;/td&gt;
      &lt;td&gt;4516&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 11:00:00&lt;/td&gt;
      &lt;td&gt;4767&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 12:00:00&lt;/td&gt;
      &lt;td&gt;5026&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;2012-10-02 13:00:00&lt;/td&gt;
      &lt;td&gt;4918&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.set_index(&#39;date_time&#39;,inplace = True)
df.sort_index()[&#39;traffic_volume&#39;].rolling(4000).mean().plot(figsize = (16,8))
plt.xlabel(&#39;Date Time&#39;)
plt.ylabel(&#39;Traffic Volume&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;rain_1h&lt;/th&gt;
      &lt;th&gt;snow_1h&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;weather_description&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;scattered clouds&lt;/td&gt;
      &lt;td&gt;5545&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;4516&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;4767&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;overcast clouds&lt;/td&gt;
      &lt;td&gt;5026&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;broken clouds&lt;/td&gt;
      &lt;td&gt;4918&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;output_39_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;removing-weather_description&#34;&gt;Removing Weather_description&lt;/h3&gt;
&lt;p&gt;I have chosen to remove weather_description due to redundancy when placing with weather_main&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.drop(columns = [&#39;weather_description&#39;], inplace = True)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;rain_1h&lt;/th&gt;
      &lt;th&gt;snow_1h&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;5545&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4516&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4767&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;5026&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4918&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;removendo-rain_1h-e-snow_1h&#34;&gt;Removendo rain_1h e snow_1h&lt;/h3&gt;
&lt;p&gt;As the distribution of rain_1h and snow_1h is not good and the vast majority of values are at zero, I will remove them, since the weather information includes whether it is raining, snowing, or neither.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.drop(columns = [&#39;rain_1h&#39;,&#39;snow_1h&#39;],inplace = True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;traffic_volume&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;5545&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4516&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4767&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;5026&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;4918&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;feature-importance&#34;&gt;Feature Importance&lt;/h3&gt;
&lt;p&gt;I will run a Random Forest in order to find the importance of each feature for the result of traffic_volume. However, it is necessary to transform categorical data first.&lt;/p&gt;
&lt;h3 id=&#34;splitting-features-and-traffic-volume&#34;&gt;Splitting Features and Traffic Volume&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;feats = df.drop(&#39;traffic_volume&#39;,1).copy()
label = df[&#39;traffic_volume&#39;].copy()

feats.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;Clouds&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;copy-with-label-enconder-of-the-features&#34;&gt;Copy with Label Enconder of the features&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;le = LabelEncoder()
num_feats = feats.copy()

for column in num_feats.columns:
    if num_feats[column].dtype == &#39;object&#39;:
        num_feats[column] = le.fit_transform(num_feats[column])

num_feats.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;holiday&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;weather_main&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Weekday&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;copy-with-one-hot-enconding-of-the-features&#34;&gt;Copy with One Hot Enconding of the features&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;onehot_feats = feats.copy()
onehot_feats[[&#39;Year&#39;,&#39;Month&#39;,&#39;Weekday&#39;]] = feats[[&#39;Year&#39;,&#39;Month&#39;,&#39;Weekday&#39;]].astype(&#39;category&#39;)
onehot_feats = pd.get_dummies(onehot_feats)

onehot_feats.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;temp&lt;/th&gt;
      &lt;th&gt;clouds_all&lt;/th&gt;
      &lt;th&gt;Hour&lt;/th&gt;
      &lt;th&gt;holiday_Holiday&lt;/th&gt;
      &lt;th&gt;holiday_None&lt;/th&gt;
      &lt;th&gt;weather_main_Clear&lt;/th&gt;
      &lt;th&gt;weather_main_Clouds&lt;/th&gt;
      &lt;th&gt;weather_main_Drizzle&lt;/th&gt;
      &lt;th&gt;weather_main_Fog&lt;/th&gt;
      &lt;th&gt;weather_main_Haze&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;Month_10&lt;/th&gt;
      &lt;th&gt;Month_11&lt;/th&gt;
      &lt;th&gt;Month_12&lt;/th&gt;
      &lt;th&gt;Weekday_0&lt;/th&gt;
      &lt;th&gt;Weekday_1&lt;/th&gt;
      &lt;th&gt;Weekday_2&lt;/th&gt;
      &lt;th&gt;Weekday_3&lt;/th&gt;
      &lt;th&gt;Weekday_4&lt;/th&gt;
      &lt;th&gt;Weekday_5&lt;/th&gt;
      &lt;th&gt;Weekday_6&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date_time&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 09:00:00&lt;/th&gt;
      &lt;td&gt;288.28&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 10:00:00&lt;/th&gt;
      &lt;td&gt;289.36&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 11:00:00&lt;/th&gt;
      &lt;td&gt;289.58&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 12:00:00&lt;/th&gt;
      &lt;td&gt;290.13&lt;/td&gt;
      &lt;td&gt;90&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2012-10-02 13:00:00&lt;/th&gt;
      &lt;td&gt;291.14&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows √ó 42 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;feature-importance---label-encoder&#34;&gt;Feature Importance - Label Encoder&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = RandomForestRegressor(random_state=1, max_depth=10)
model.fit(num_feats,label)

#Plot
features = feats.columns
importances = model.feature_importances_
indices = np.argsort(importances)[-5:] #Top 5 features
plt.title(&#39;Feature Importances&#39;)
plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;)
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel(&#39;Relative Importance&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_53_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;feature-importance---one-hot-encoding&#34;&gt;Feature Importance - One Hot Encoding&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = RandomForestRegressor(random_state=1, max_depth=10)
model.fit(onehot_feats,label)
#Plot t
features = onehot_feats.columns
importances = model.feature_importances_
indices = np.argsort(importances)[-10:] #Top 10 features
plt.title(&#39;Feature Importances&#39;)
plt.barh(range(len(indices)), importances[indices], color=&#39;b&#39;, align=&#39;center&#39;)
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel(&#39;Relative Importance&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is clear now that the hour feature is the one that most influences the traffic volume. This can be commonly observed in big cities where at rush hour the traffic volume is high.&lt;/p&gt;
&lt;h2 id=&#34;scaling&#34;&gt;Scaling&lt;/h2&gt;
&lt;p&gt;Necessary to keep the numerical values in a range and consequently one feature does not stand out much more than the other when training the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = num_feats.values
X = preprocessing.scale(X)
y = label.values

print(X)
print(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[ 0.03560205  0.53041592 -0.24004863 ...  1.02779521 -0.98947829
  -0.34548099]
 [ 0.03560205  0.61138394  0.65704665 ...  1.02779521 -0.98947829
  -0.20139502]
 [ 0.03560205  0.62787742  1.04151605 ...  1.02779521 -0.98947829
  -0.05730906]
 ...
 [ 0.03560205  0.11433026  1.04151605 ...  0.73369913  1.50301966
   1.38355059]
 [ 0.03560205  0.06634921  1.04151605 ...  0.73369913  1.50301966
   1.52763656]
 [ 0.03560205  0.06859832  1.04151605 ...  0.73369913  1.50301966
   1.67172252]]
[5545 4516 4767 ... 2159 1450  954]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;train-and-test&#34;&gt;Train and Test&lt;/h2&gt;
&lt;h3 id=&#34;holdout07-03&#34;&gt;Holdout(0.7; 0.3)&lt;/h3&gt;
&lt;p&gt;The dataset will be randomly split between training and testing. For this task, we will divide 70% for training, while the rest will be used for testing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)
X_train.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(33730, 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;random-forest&#34;&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;It is a supervised ML(Machine Learning) algorithm. Creates several decision trees and combines them to obtain a better prediction.
&lt;br/&gt;
&lt;br/&gt;
Random forest adds randomness to the model when creating trees. It searches for the best attribute in a random subset of the attribute. This creates diversity and generates better models. For this reason, it was possible to find the importance of attributes, performed previously.
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;Advantages:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can be used for classification and regression.&lt;/li&gt;
&lt;li&gt;Handles missing values well.&lt;/li&gt;
&lt;li&gt;It will hardly overfit the model.&lt;/li&gt;
&lt;li&gt;Handles large, high dimensional data sets very well (Main reason for choosing this method)
&lt;br/&gt;
&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The programmer does not have much control on the model, it generates black box.
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Random Forest Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div&gt;
&lt;img src=&#34;random_forest.png&#34; width=&#34;500&#34;/&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg = RandomForestRegressor()
reg = reg.fit(X_train,y_train)
predict = reg.predict(X_test)
accuracy = reg.score(X_test, y_test)
mae = mean_absolute_error(y_test, predict)

print(f&amp;quot;R2 Score: {round(accuracy * 100, 3)}%&amp;quot;)
print(f&amp;quot;Mean Absolute Error: {round(mae, 4)}&amp;quot;)
print(f&#39;RMSE: {round(np.sqrt(mean_squared_error(y_pred=predict,y_true=y_test)),4)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R2 Score: 95.901%
Mean Absolute Error: 221.4494
RMSE: 405.0598
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;xgboost&#34;&gt;XGBoost&lt;/h3&gt;
&lt;p&gt;It is an implementation of the Gradient Boost supervised ML algorithm, famous for its speed and performance. Gradient Boost tries to predict a target variable, for this it combines a group of estimates from a set of simpler and &amp;lsquo;weaker&amp;rsquo; models and transforms them into a &amp;lsquo;stronger&amp;rsquo; model.
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;Gradient Boosting Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div&gt;
&lt;img src=&#34;gradboost.png&#34; width=&#34;350&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;In this example, it can be seen that Gradient Boosting is a method in which new models are trained to resolve errors from previous models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xgb_model = xgb.XGBRegressor(objective=&amp;quot;reg:squarederror&amp;quot;,  n_estimators=1000, random_state=42,n_jobs=-1)
xgb_model.fit(X_train,y_train,early_stopping_rounds=10,eval_set=[(X_test, y_test)],verbose = 0)
y_pred = xgb_model.predict(X_test)

xgb_mae = mean_absolute_error(y_test, y_pred)
print(f&#39;R2 Score: {round(xgb_model.score(X_test,y_test)*100,2)}&#39;)
print(f&#39;RMSE: {np.sqrt(mean_squared_error(y_pred=y_pred,y_true=y_test))}&#39;)
print(f&amp;quot;Mean Absolute Error: {round(xgb_mae, 4)}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;R2 Score: 96.19
RMSE: 390.32545493877905
Mean Absolute Error: 232.6679
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cross-validation&#34;&gt;Cross-Validation&lt;/h2&gt;
&lt;p&gt;In this method, the dataset is divided into K parts, while K-1 parts are used for training the remaining partition is used for testing until all the dataset is utilized. The validation of each forecast made is saved in a list and then the average of each metric will be made.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;random-forest-1&#34;&gt;Random Forest&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rmse = []
r2 = []
mae_l = []
for train_index, test_index in kf.split(X):
    reg_kf = RandomForestRegressor()
    reg_kf.fit(X[train_index],y[train_index])
    y_pred = reg_kf.predict(X[test_index])
    actuals = y[test_index]
    rmse.append(np.sqrt(mean_squared_error(actuals, y_pred)))
    r2.append(r2_score(actuals,y_pred))
    mae_l.append(mean_absolute_error(actuals, y_pred))


avg_r2 = np.mean(r2)
avg_rmse = np.mean(rmse)
avg_mae = np.mean(mae_l)
print(f&#39;AVG R2 Score :{round(avg_r2,3)*100}%\t Average RMSE:{avg_rmse}\t Average MAE:{round(avg_mae, 4)}&#39;)    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AVG R2 Score :95.7%	 Average RMSE:409.69403930472544	 Average MAE:217.4925
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;xgboost-1&#34;&gt;XGBoost&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rmse = []
r2 = []
mae_l = []
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBRegressor(objective=&amp;quot;reg:squarederror&amp;quot;,  n_estimators=1000, random_state=42,n_jobs=-1)
    xgb_model.fit(X[train_index],y[train_index],early_stopping_rounds=10,eval_set=[(X[test_index], y[test_index])], verbose = 0)
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    rmse.append(np.sqrt(mean_squared_error(actuals, predictions)))
    r2.append(r2_score(actuals,predictions))
    mae_l.append(mean_absolute_error(actuals, predictions))

avg_r2 = np.mean(r2)
avg_rmse = np.mean(rmse)
avg_mae = np.mean(mae_l)
print(f&#39;AVG R2 Score :{round(avg_r2,3)*100}%\t Average RMSE:{avg_rmse}\t Average MAE:{round(avg_mae, 4)}&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AVG R2 Score :96.1%	 Average RMSE:390.5595082859465	 Average MAE:229.8818
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;results-and-discussion&#34;&gt;Results and Discussion:&lt;/h2&gt;
&lt;p&gt;The metrics chosen for the evaluation of the models were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RMSE(Root Mean Squared Error), average of the difference between the actual and the predicted value.&lt;/li&gt;
&lt;li&gt;MAE(Mean Absolute Error), module mean of the difference between the actual and the predicted value.&lt;/li&gt;
&lt;li&gt;R¬≤(R Squared), it is a measure that demonstrates how much the model explains the variance of the real value. It varies between zero and one, the closer to one the more adjusted to the sample is the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; From the results presented for each model, it is noticeable that the forecasts fit well with the expected sample. The Holdout method showed better efficiency for the problem than the cross-validation, although the results are not so different. The XGBoost algorithm obtained forecasts with less error and more adjusted to the original sample.
&lt;br/&gt;
&lt;p&gt; Despite the satisfying result of the predictive models, they may not be efficient for certain applications, like a route recommendation system, where the value must be very precise. To solve this problem, the next works are expected to implement specific models for time-based problems, such as Recurrent Neural Networks (RNN) and ARIMA, since the features that are most relevant to the problem are related to time series.
</description>
    </item>
    
  </channel>
</rss>
